{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prompts\n",
    "\n",
    "Prompts are essentially the instructions or questions you give to AI models to get them to generate specific responses.\\\n",
    "Think of them as the conversation starter that guides what the AI will say back to you.\n",
    "\n",
    "This has even given rise to a field called \"Prompt Engineering\" — a discipline focused on developing and optimizing prompts for effective use of language models.\n",
    "\n",
    "In this notebook, we'll explore prompts in Spring AI:\n",
    "* Basic prompts\n",
    "* Message types\n",
    "* Prompting techniques\n",
    "\n",
    "Let's add the necessary dependencies:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:15:40.367899Z",
     "start_time": "2026-02-19T22:15:39.973689Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_3_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-anthropic"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To use the model, we need to provide an API key.\n",
    "\n",
    "You can obtain this API key from\n",
    "[console.anthropic.com](https://console.anthropic.com/settings/keys)\n",
    "for Anthropic models or from\n",
    "[platform.openai.com](https://platform.openai.com/api-keys)\n",
    "for OpenAI models.\n",
    "\n",
    "Then add the generated API key to your environment variables:\n",
    "\n",
    "[MacOS/Linux]\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "export OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "\n",
    "```\n",
    "\n",
    "[Windows]\n",
    "```shell\n",
    "set ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "set OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "```\n",
    "\n",
    "Let's retrieve the API key from environment variables:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:15:40.438771Z",
     "start_time": "2026-02-19T22:15:40.369866Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_4_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "val apiKey = System.getenv(\"ANTHROPIC_API_KEY\") ?: \"YOUR_ANTHROPIC_API_KEY\"",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Just like in the **`Intro`** notebook, let's create `ChatOptions` and a `ChatModel`."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:15:40.921423Z",
     "start_time": "2026-02-19T22:15:40.442703Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_5_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicApi = AnthropicApi.builder().apiKey(apiKey).build()\n",
    "val anthropicOptions = AnthropicChatOptions.builder()\n",
    "    .model(AnthropicApi.ChatModel.CLAUDE_SONNET_4_5)\n",
    "    .temperature(0.7)\n",
    "    .maxTokens(1024)\n",
    "    .build()\n",
    "\n",
    "val chatCompletion = AnthropicChatModel.builder()\n",
    "    .anthropicApi(anthropicApi)\n",
    "    .defaultOptions(anthropicOptions)\n",
    "    .build()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What makes a prompt?\n",
    "\n",
    "A prompt is simply a text request: \"tell me a joke\" or \"write a poem about mountains\"\n",
    "\n",
    "Let's ask our LLM to generate a haiku:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:15:43.379240Z",
     "start_time": "2026-02-19T22:15:40.922288Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_6_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "chatCompletion.call(\"Generate a hokku\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Hokku\n",
       "\n",
       "Morning dew trembles—\n",
       "a spider's silk catches light\n",
       "between two pine boughs"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we're using `ChatClient` and the `Prompt` class, the request would look like this:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:15:45.596169Z",
     "start_time": "2026-02-19T22:15:43.380070Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_7_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val chatClient = ChatClient.create(chatCompletion)\n",
    "\n",
    "val prompt = Prompt(\"Generate a hokku\")\n",
    "chatClient.prompt(prompt).call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Hokku\n",
       "\n",
       "Winter moon rising—\n",
       "the bare branch casts its shadow\n",
       "across fresh-fallen snow"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Type of messages\n",
    "\n",
    "In AI interactions, there are several message types (roles):\n",
    "* User — message from the user\n",
    "* Assistant — message from the AI\n",
    "* System — instructions that guide the AI's behavior\n",
    "* Tool — used for function calling"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### User messages\n",
    "\n",
    "We've been writing user messages all along.\n",
    "\n",
    "Let's explicitly define them now:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:15:48.847006Z",
     "start_time": "2026-02-19T22:15:45.600709Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_8_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val messages = Prompt(UserMessage(\"Generate a hokku\"), UserMessage(\"what's name of this hokku?\"))\n",
    "chatClient.prompt(messages).call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Autumn Moon Rising\n",
       "\n",
       "Autumn moon rising—\n",
       "a single cricket's last song\n",
       "fades into silence\n",
       "\n",
       "---\n",
       "\n",
       "This hokku is called **\"Autumn Moon Rising\"** (taken from its opening line, following traditional practice)."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### System messages\n",
    "\n",
    "A system message tells the LLM how it should behave.\n",
    "In Spring-AI, you can send a system message in several ways.\n",
    "\n",
    "For instance, you can use a special function for `ChatClient` or create an instance of a system message and pass it to the LLM directly."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:15:56.483599Z",
     "start_time": "2026-02-19T22:15:48.848021Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_9_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt()\n",
    "    .system(\"You are a financial expert. Answer briefly.\")\n",
    "    .user(\"If I had a time machine, should I buy Bitcoin in 2011?\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**Yes, absolutely** — from a purely financial perspective.\n",
       "\n",
       "**The numbers:**\n",
       "- 2011 Bitcoin price: ~$1-$30\n",
       "- 2021 peak: ~$69,000\n",
       "- Current (~2024): ~$40,000-$100,000+\n",
       "\n",
       "A $1,000 investment in early 2011 could have turned into **$2-70 million** at the peak.\n",
       "\n",
       "**However, real-world challenges:**\n",
       "- **Holding psychology**: Most early buyers sold way before the peak\n",
       "- **Security risks**: Many early exchanges were hacked or collapsed (Mt. Gox)\n",
       "- **Wallet management**: Countless people lost access to their Bitcoin\n",
       "- **Tax implications**: Massive capital gains taxes on profits\n",
       "\n",
       "**Optimal strategy with a time machine:**\n",
       "- Buy in 2011-2012\n",
       "- Store securely in cold storage\n",
       "- Sell portions at major peaks (2013, 2017, 2021)\n",
       "- Diversify profits into other assets\n",
       "\n",
       "The real challenge wouldn't be buying — it would be having the discipline to hold through 80%+ crashes and not lose your private keys."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:04.280818Z",
     "start_time": "2026-02-19T22:15:56.484271Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_10_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val messages = listOf(\n",
    "    SystemMessage(\"You are a financial expert. Answer briefly.\"),\n",
    "    UserMessage(\"If I had a time machine, should I buy Bitcoin in 2011?\")\n",
    ")\n",
    "\n",
    "chatClient.prompt(Prompt(messages)).call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**Yes, absolutely** - from a pure financial perspective.\n",
       "\n",
       "Bitcoin in 2011 traded for roughly **$1-$30**. At its peak (2021), it reached **~$69,000** - a potential 2,300x to 69,000x return.\n",
       "\n",
       "**However, practical challenges:**\n",
       "- **Holding psychology**: Could you really hold through 80%+ crashes (2011, 2014, 2018)?\n",
       "- **Security**: Early exchanges were hacked/collapsed (Mt. Gox lost 850,000 BTC)\n",
       "- **Access**: You'd need to safely store private keys for 10+ years\n",
       "- **Selling timing**: Knowing *when* to exit would be crucial\n",
       "\n",
       "**Better strategy if you had a time machine:**\n",
       "- Buy in 2011\n",
       "- Sell portions at multiple peaks (2013, 2017, 2021)\n",
       "- Secure proper cold storage immediately\n",
       "\n",
       "The biggest risk wouldn't be Bitcoin failing - it would be losing access to your coins or panic-selling too early."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Assistant messages\n",
    "\n",
    "As mentioned earlier, an assistant message is essentially a message from the LLM.\n",
    "\n",
    "Let's create a simple conversation example:\n",
    "we'll give the LLM a system instruction,\n",
    "send a request,\n",
    "and after receiving a response,\n",
    "ask for clarification on a specific point."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:06.790385Z",
     "start_time": "2026-02-19T22:16:04.281351Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_11_jupyter",
      "Line_12_jupyter",
      "Line_13_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val messages = mutableListOf(\n",
    "    SystemMessage(\"You are an assistant who always answers very briefly, using no more than 10 words.\"),\n",
    "    UserMessage(\"Tell me about the capital of France and its landmarks\")\n",
    ")\n",
    "val assistantMessage = chatClient.prompt(Prompt(messages.toList())).call().chatResponse()!!.result?.output\n",
    "assistantMessage"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantMessage [messageType=ASSISTANT, toolCalls=[], textContent=Paris is France's capital. Famous landmarks include Eiffel Tower, Louvre., metadata={messageType=ASSISTANT}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:08.614678Z",
     "start_time": "2026-02-19T22:16:06.797210Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_14_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "messages.add(assistantMessage)\n",
    "messages.add(UserMessage(\"Tell me about the third landmark\"))\n",
    "\n",
    "chatClient.prompt(Prompt(messages.toList())).call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I only mentioned two landmarks: Eiffel Tower and Louvre Museum."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We've essentially created a simple dialogue between human and machine.\n",
    "\n",
    "Notice that in the last message,\n",
    "we deliberately formulated the request as:\n",
    "`\"Tell me about the third landmark\"`.\n",
    "If we hadn't sent this request along with the LLM's response about French landmarks,\n",
    "we wouldn't have received a meaningful answer."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt templates\n",
    "\n",
    "Spring AI provides PromptTemplate for working with prompts. This class uses the OSS\n",
    "[String Template](https://www.stringtemplate.org/)\n",
    "engine developed by Terence Parr for constructing and managing prompts.\n",
    "\n",
    "This class allows you to use resources for prompts, making them easier to manage and localize in different languages.\n",
    "It also lets you insert your data into the prompt,\n",
    "which is especially useful when developing RAG applications (which we'll learn more about in future notebooks).\n",
    "\n",
    "Let's write a simple example using PromptTemplate:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:11.275849Z",
     "start_time": "2026-02-19T22:16:08.617300Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_15_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "fun capital(): PromptTemplate {\n",
    "    val message = \"The two largest cities in {country}\"\n",
    "    return PromptTemplate(message)\n",
    "}\n",
    "\n",
    "val prompt = capital().create(mapOf(\"country\" to \"France\"))\n",
    "chatClient.prompt(prompt).call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The two largest cities in France are:\n",
       "\n",
       "1. **Paris** - The capital and by far the largest city, with a population of approximately 2.2 million in the city proper and over 12 million in the metropolitan area.\n",
       "\n",
       "2. **Marseille** - The second-largest city, located on the Mediterranean coast, with a population of approximately 870,000 in the city proper and around 1.8 million in the metropolitan area."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompting techniques\n",
    "\n",
    "There are various techniques for crafting effective prompts that help us get better results from language models.\n",
    "These techniques range from simple to complex and can dramatically improve the quality of AI responses.\n",
    "\n",
    "We will consider techniques such as:\n",
    "* Zero-Shot Prompting\n",
    "* Few-Shot Prompting\n",
    "* Chain-of-Thought (CoT) Prompting\n",
    "* Meta Prompting\n",
    "* Generate Knowledge Prompting\n",
    "* Prompt Chaining\n",
    "\n",
    "These are far from all the techniques. There are many more."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting is a technique where the model performs a task based on direct instruction without being provided examples or demonstrations.\n",
    "The model relies solely on its pre-training knowledge."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:14.085959Z",
     "start_time": "2026-02-19T22:16:11.281030Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_16_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Classify the text as neutral, negative, or positive.\n",
    "    Text: In my opinion, this restaurant is quite ordinary.\n",
    "    Tonality:\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tonality: **Negative** (leaning neutral)\n",
       "\n",
       "The word \"ordinary\" suggests the restaurant is unremarkable or mediocre, which carries a mildly negative connotation. The phrase \"in my opinion\" softens it slightly, but overall the sentiment expresses disappointment or lack of enthusiasm rather than satisfaction."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting is a technique where several examples (demonstrations) of task performance are included in the prompt to help the model understand exactly how to perform a similar task.\n",
    "Instead of training the model from scratch, we provide context through examples directly in the prompt."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:16.823914Z",
     "start_time": "2026-02-19T22:16:14.086653Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_17_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    \"Zumbrik\" is a small fluffy animal inhabiting the Altai Mountains. Example sentence with the word zumbrik:\n",
    "    During our expedition to the Altai Mountains we met a family of cute zumbriks.\n",
    "\n",
    "    \"Fyrkotat\" means to quickly rotate in one place. Example sentence with the word fyrkotat:\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Looking at the pattern from the first example, I'll create a natural sentence using \"fyrkotat\":\n",
       "\n",
       "The dancer began to fyrkotat across the stage, spinning rapidly before coming to a sudden stop."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, we provided the model with one example (1-shot) of using a made-up word in a sentence.\n",
    "Based on this,\n",
    "the model understood the task and was able to create a similar sentence with another made-up word,\n",
    "following the demonstrated pattern."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain-of-Thought is a prompting technique that encourages the model to show intermediate steps of reasoning before providing the final answer.\n",
    "This is especially useful for complex tasks requiring mathematical calculations,\n",
    "logical analysis, or multi-step reasoning."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:20.950466Z",
     "start_time": "2026-02-19T22:16:16.829978Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_18_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    In the group of numbers 15, 8, 3, 22, 7, 14, 26, do the odd numbers sum to an even number?\n",
    "    Let's reason step by step.\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I need to find the odd numbers in the group and then sum them to see if the result is even.\n",
       "\n",
       "**Step 1: Identify the odd numbers**\n",
       "\n",
       "Let me go through each number:\n",
       "- 15 → odd\n",
       "- 8 → even\n",
       "- 3 → odd\n",
       "- 22 → even\n",
       "- 7 → odd\n",
       "- 14 → even\n",
       "- 26 → even\n",
       "\n",
       "The odd numbers are: 15, 3, 7\n",
       "\n",
       "**Step 2: Sum the odd numbers**\n",
       "\n",
       "15 + 3 + 7 = 25\n",
       "\n",
       "**Step 3: Determine if the sum is even**\n",
       "\n",
       "25 is an odd number (not even).\n",
       "\n",
       "**Answer: No, the odd numbers sum to 25, which is an odd number, not an even number.**"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Meta Prompting\n",
    "\n",
    "Meta prompting is an advanced technique that focuses on the structural and syntactic aspects of tasks rather than specific content details.\n",
    "It creates an abstract, structured way of interacting with the LLM,\n",
    "where the form and pattern of information are more important than the content itself."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:27.755821Z",
     "start_time": "2026-02-19T22:16:20.953971Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_19_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    In problems of the format [problem P → solution S], follow this structure:\n",
    "    1. Define the variables from P\n",
    "    2. Construct an equation based on P\n",
    "    3. Solve the equation to find S\n",
    "    4. Verify the solution by substitution\n",
    "\n",
    "    Problem: A store had x apples. After selling 15 apples and then another 1/3 of the remaining apples, the store had 20 apples left. How many apples were there initially?\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I'll solve this step-by-step following the given structure.\n",
       "\n",
       "## 1. Define the variables from P\n",
       "\n",
       "Let x = the initial number of apples in the store\n",
       "\n",
       "## 2. Construct an equation based on P\n",
       "\n",
       "Let me trace through what happened:\n",
       "- Started with: x apples\n",
       "- After selling 15 apples: (x - 15) apples remain\n",
       "- Then sold 1/3 of the remaining: sold (1/3)(x - 15) apples\n",
       "- After this second sale: (x - 15) - (1/3)(x - 15) apples remain\n",
       "- This equals 20 apples\n",
       "\n",
       "The equation is:\n",
       "**(x - 15) - (1/3)(x - 15) = 20**\n",
       "\n",
       "## 3. Solve the equation to find S\n",
       "\n",
       "(x - 15) - (1/3)(x - 15) = 20\n",
       "\n",
       "Factor out (x - 15):\n",
       "(x - 15)[1 - 1/3] = 20\n",
       "\n",
       "(x - 15)(2/3) = 20\n",
       "\n",
       "Multiply both sides by 3/2:\n",
       "x - 15 = 20 × (3/2)\n",
       "\n",
       "x - 15 = 30\n",
       "\n",
       "x = 45\n",
       "\n",
       "**Solution: There were initially 45 apples.**\n",
       "\n",
       "## 4. Verify the solution by substitution\n",
       "\n",
       "Starting with x = 45:\n",
       "- Initial apples: 45\n",
       "- After selling 15: 45 - 15 = 30 apples\n",
       "- Sell 1/3 of remaining: (1/3) × 30 = 10 apples sold\n",
       "- Final amount: 30 - 10 = 20 apples ✓\n",
       "\n",
       "The solution checks out correctly."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, the meta prompt sets a general structure for approaching problem-solving,\n",
    "not focusing on specific content but offering a universal template for analysis and solution.\n",
    "The model follows this structure, applying it to the specific problem."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate Knowledge Prompting\n",
    "\n",
    "Generate Knowledge Prompting is a technique where the model first generates factual knowledge about a topic and then uses this knowledge to form a more accurate and well-founded answer.\n",
    "This technique is especially useful for tasks requiring common sense or factual accuracy."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:37.423268Z",
     "start_time": "2026-02-19T22:16:27.756736Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_20_jupyter",
      "Line_21_jupyter",
      "Line_22_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val knowledge = chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Request: Are lichens harmful to trees?\n",
    "    Generate knowledge:\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()\n",
    "\n",
    "knowledge"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Generated Knowledge: Are Lichens Harmful to Trees?\n",
       "\n",
       "## What Are Lichens?\n",
       "Lichens are composite organisms formed from a symbiotic relationship between fungi and algae or cyanobacteria. They appear as crusty, leafy, or branching growths on tree bark, rocks, and other surfaces.\n",
       "\n",
       "## Relationship Between Lichens and Trees\n",
       "\n",
       "### Lichens Are Not Parasitic\n",
       "- Lichens do not penetrate tree bark or extract nutrients from the tree itself\n",
       "- They are **epiphytes** - organisms that grow on plants without harming them\n",
       "- Lichens obtain their nutrients from air, rain, and photosynthesis, not from host trees\n",
       "\n",
       "### Why Lichens Grow on Trees\n",
       "- Tree bark provides a stable surface for attachment\n",
       "- Lichens use trees merely as a physical support structure\n",
       "- They do not have roots that penetrate into the tree's vascular system\n",
       "\n",
       "## Common Misconceptions\n",
       "\n",
       "### Correlation vs. Causation\n",
       "- Lichens are often observed on declining or dead trees, leading to the mistaken belief they caused the decline\n",
       "- In reality, lichens grow more abundantly on stressed trees because:\n",
       "  - Thinning canopy allows more light to reach the bark\n",
       "  - Slower bark growth on unhealthy trees provides stable surfaces\n",
       "  - Lichens are indicators of existing problems, not the cause\n",
       "\n",
       "## Benefits of Lichens\n",
       "- Serve as **bioindicators** of air quality (sensitive to pollution)\n",
       "- Provide habitat and food for insects and wildlife\n",
       "- Contribute to ecosystem biodiversity\n",
       "- Do not compete with trees for resources\n",
       "\n",
       "## Conclusion\n",
       "**Lichens are not harmful to trees.** They are harmless organisms that simply use tree bark as a growing surface."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:41.268648Z",
     "start_time": "2026-02-19T22:16:37.434790Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_23_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Answer briefly: Are lichens harmful to trees?\n",
    "\n",
    "    knowledge: $knowledge\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Brief Answer\n",
       "\n",
       "**No, lichens are not harmful to trees.** \n",
       "\n",
       "Lichens are epiphytes that simply use tree bark as a physical support structure. They don't penetrate the bark, extract nutrients from the tree, or damage it in any way. They get their nutrients from air, rain, and photosynthesis instead.\n",
       "\n",
       "While lichens are often seen on declining trees, they don't cause the decline—they're just more visible on stressed trees due to thinning canopies and slower bark growth. They're harmless indicators, not the problem."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This technique allows the model to first gather relevant knowledge and then use it to form a more accurate and informative response."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompt Chaining\n",
    "\n",
    "Prompt Chaining is a technique where a complex task is broken down into several sequential subtasks.\n",
    "The answer from one prompt becomes the input data for the next, creating a chain of operations.\n",
    "This allows solving complex tasks, increases transparency, controllability, and reliability when working with LLMs."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:43.240914Z",
     "start_time": "2026-02-19T22:16:41.270771Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_24_jupyter",
      "Line_25_jupyter",
      "Line_26_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val ctryCap = chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Extract all mentions of countries and their capitals from the following text.  The answer should be a list in the format \"Country: Capital\".\n",
    "\n",
    "    Text:\n",
    "    France is famous for the Eiffel Tower in Paris, and Germany is known for its automotive industry with headquarters in Berlin.  Meanwhile, tourists enjoy the picturesque views of Rome in Italy and the castles near Madrid in Spain.\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:16:53.463458Z",
     "start_time": "2026-02-19T22:16:43.242649Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_27_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"\"\"\n",
    "    Based on the following list of countries and their capitals, create a short guide to the three most interesting sights in each capital city:\n",
    "\n",
    "    $ctryCap\n",
    "    \"\"\")\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Guide to Capital City Sights\n",
       "\n",
       "## Paris, France\n",
       "\n",
       "1. **The Eiffel Tower** - This iconic iron lattice tower offers breathtaking views of Paris from its observation decks and is especially magical when illuminated at night.\n",
       "\n",
       "2. **The Louvre Museum** - Home to thousands of works of art including the Mona Lisa and Venus de Milo, this former royal palace is the world's largest art museum.\n",
       "\n",
       "3. **Notre-Dame Cathedral** - A masterpiece of Gothic architecture featuring stunning rose windows, flying buttresses, and gargoyles (currently under restoration following the 2019 fire).\n",
       "\n",
       "## Berlin, Germany\n",
       "\n",
       "1. **Brandenburg Gate** - This neoclassical monument has witnessed pivotal moments in German history and stands as a powerful symbol of reunification.\n",
       "\n",
       "2. **The Berlin Wall Memorial** - Preserved sections of the Wall and documentation center offer sobering insights into the city's divided past.\n",
       "\n",
       "3. **Museum Island** - A UNESCO World Heritage site housing five world-renowned museums, including the Pergamon Museum with its ancient architectural treasures.\n",
       "\n",
       "## Rome, Italy\n",
       "\n",
       "1. **The Colosseum** - This ancient amphitheater, nearly 2,000 years old, once hosted gladiatorial contests and remains an awe-inspiring testament to Roman engineering.\n",
       "\n",
       "2. **The Vatican Museums & Sistine Chapel** - Marvel at Michelangelo's ceiling frescoes and explore one of the world's greatest art collections.\n",
       "\n",
       "3. **The Trevi Fountain** - This baroque masterpiece is Rome's largest fountain; toss a coin to ensure your return to the Eternal City.\n",
       "\n",
       "## Madrid, Spain\n",
       "\n",
       "1. **Prado Museum** - One of the world's finest art galleries, featuring masterpieces by Velázquez, Goya, and El Greco.\n",
       "\n",
       "2. **Royal Palace of Madrid** - Western Europe's largest royal palace boasts opulent rooms, armories, and beautiful gardens open for public tours.\n",
       "\n",
       "3. **Retiro Park** - This expansive green oasis in the city center features a stunning Crystal Palace, boating lake, and peaceful gardens perfect for strolling."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Working with prompts is fundamental for both AI application users and developers.\n",
    "Spring AI and Kotlin provide a convenient and powerful API for this purpose.\n",
    "\n",
    "Check out the next notebook to learn more about Kotlin and Spring AI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
