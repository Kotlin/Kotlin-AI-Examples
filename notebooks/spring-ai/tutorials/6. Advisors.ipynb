{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advisors\n",
    "\n",
    "Advisors are an interesting feature in Spring-AI that allows you to flexibly intercept,\n",
    "modify, and enhance AI interactions.\n",
    "\n",
    "With `Advisors`, you can:\n",
    "- Add necessary context to user requests\n",
    "- Filter out harmful or sensitive content in AI requests\n",
    "- Track custom metrics\n",
    "- Ensure consistent output structure\n",
    "- And more\n",
    "\n",
    "Let's add dependencies and create a `ChatModel`"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:40:17.781009Z",
     "start_time": "2026-02-19T22:40:17.379219Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_3_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-openai"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:40:18.321321Z",
     "start_time": "2026-02-19T22:40:17.781511Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_4_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "val openAiApi = OpenAiApi.builder().apiKey(apiKey).build()\n",
    "val openAiOptions = OpenAiChatOptions.builder()\n",
    "    .model(OpenAiApi.ChatModel.GPT_5_CHAT_LATEST)\n",
    "    .temperature(0.7)\n",
    "    .build()\n",
    "\n",
    "\n",
    "val chatModel = OpenAiChatModel.builder()\n",
    "    .openAiApi(openAiApi)\n",
    "    .defaultOptions(openAiOptions)\n",
    "    .build()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's add the `MessageChatMemory` advisor.\n",
    "As the name suggests, this advisor will implement message history,\n",
    "preserving the conversation context.\n",
    "For this Advisor, we'll need a `ChatMemory` instance\n",
    "where messages will be stored."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:40:18.414576Z",
     "start_time": "2026-02-19T22:40:18.321997Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_5_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val chatMemory = MessageWindowChatMemory.builder().build()\n",
    "\n",
    "val chatClient = ChatClient\n",
    "    .builder(chatModel)\n",
    "    .defaultAdvisors(MessageChatMemoryAdvisor.builder(chatMemory).build())\n",
    "    .build()\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's test how this works"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:40:19.668175Z",
     "start_time": "2026-02-19T22:40:18.424755Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_6_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "chatClient.prompt(\"Hi, tell me a joke\").call().content()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sure! Hereâ€™s one for you:  \n",
       "\n",
       "Why donâ€™t skeletons fight each other?  \n",
       "\n",
       "Because they donâ€™t have the guts. ðŸ¦´ðŸ˜„"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:40:20.693789Z",
     "start_time": "2026-02-19T22:40:19.673083Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_7_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": "chatClient.prompt(\"What is previous message in our chat history?\").call().content()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The previous message in our chat history is the joke I just told you:  \n",
       "\n",
       "\"Why donâ€™t skeletons fight each other? Because they donâ€™t have the guts.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, the LLM now has access to our message history.\n",
    "\n",
    "Spring-AI includes several predefined Advisors:\n",
    "- `MessageChatMemoryAdvisor`\n",
    "- `PromptChatMemoryAdvisor`\n",
    "- `QuestionAnswerAdvisor`\n",
    "- `RetrievalAugmentationAdvisor`\n",
    "- `SafeGuardAdvisor`\n",
    "- `SimpleLoggerAdvisor`\n",
    "- `VectorStoreChatMemoryAdvisor`\n",
    "\n",
    "And you can create your own custom Advisor as well.\n",
    "\n",
    "Let's do that now.\n",
    "We'll create an Advisor that logs requests and responses by outputting them to our console.\n",
    "To do this, we'll extend `CallAroundAdvisor` and implement the `aroundCall` method"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:40:20.801865Z",
     "start_time": "2026-02-19T22:40:20.694592Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_8_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.client.advisor.api.CallAdvisor\n",
    "import org.springframework.ai.chat.client.advisor.api.CallAdvisorChain\n",
    "\n",
    "class CustomLogger: CallAdvisor {\n",
    "    override fun getName(): String {\n",
    "        return \"CustomLogger\"\n",
    "    }\n",
    "\n",
    "    override fun getOrder(): Int = 0\n",
    "\n",
    "    override fun adviseCall(chatClientRequest: ChatClientRequest, callAdvisorChain: CallAdvisorChain): ChatClientResponse {\n",
    "        println(\"CustomLogger.Before: ${chatClientRequest}\")\n",
    "        val chatClientResponse = callAdvisorChain.nextCall(chatClientRequest)\n",
    "        println(\"CustomLogger.After: ${chatClientResponse}\")\n",
    "        return chatClientResponse\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's see our `CustomAdvisor` in action"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:40:21.610851Z",
     "start_time": "2026-02-19T22:40:20.808462Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_9_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "chatClient\n",
    "    .prompt(\"Generate HelloWorld in Kotlin\")\n",
    "    .advisors(CustomLogger())\n",
    "    .call()\n",
    "    .content()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomLogger.Before: ChatClientRequest[prompt=Prompt{messages=[UserMessage{content='Hi, tell me a joke', metadata={messageType=USER}, messageType=USER}, AssistantMessage [messageType=ASSISTANT, toolCalls=[], textContent=Sure! Hereâ€™s one for you:  \n",
      "\n",
      "Why donâ€™t skeletons fight each other?  \n",
      "\n",
      "Because they donâ€™t have the guts. ðŸ¦´ðŸ˜„, metadata={role=ASSISTANT, messageType=ASSISTANT, finishReason=STOP, refusal=, index=0, annotations=[], id=chatcmpl-DB6ywKthy6COgW1i2xI9b7IqXqr8L}], UserMessage{content='What is previous message in our chat history?', metadata={messageType=USER}, messageType=USER}, AssistantMessage [messageType=ASSISTANT, toolCalls=[], textContent=The previous message in our chat history is the joke I just told you:  \n",
      "\n",
      "\"Why donâ€™t skeletons fight each other? Because they donâ€™t have the guts.\", metadata={role=ASSISTANT, messageType=ASSISTANT, finishReason=STOP, refusal=, index=0, annotations=[], id=chatcmpl-DB6yxncMeSQ4K72dn6eA5qMLFc1lD}], UserMessage{content='Generate HelloWorld in Kotlin', metadata={messageType=USER}, messageType=USER}], modelOptions=OpenAiChatOptions: {\"streamUsage\":false,\"model\":\"gpt-5-chat-latest\",\"temperature\":0.7}}, context={}]\n",
      "CustomLogger.After: ChatClientResponse[chatResponse=ChatResponse [metadata={ id: chatcmpl-DB6yyg8JWUautoLd4rCeuXk3mrHDp, usage: DefaultUsage{promptTokens=109, completionTokens=50, totalTokens=159}, rateLimit: { @type: org.springframework.ai.openai.metadata.OpenAiRateLimit, requestsLimit: 15000, requestsRemaining: 14999, requestsReset: PT0.004S, tokensLimit: 40000000; tokensRemaining: 39999904; tokensReset: PT0S } }, generations=[Generation[assistantMessage=AssistantMessage [messageType=ASSISTANT, toolCalls=[], textContent=Sure! Hereâ€™s a simple **Hello World** program written in Kotlin:\n",
      "\n",
      "```kotlin\n",
      "fun main() {\n",
      "    println(\"Hello, World!\")\n",
      "}\n",
      "```\n",
      "\n",
      "When you run this program, it will print:\n",
      "\n",
      "```\n",
      "Hello, World!\n",
      "```, metadata={role=ASSISTANT, messageType=ASSISTANT, finishReason=STOP, refusal=, index=0, annotations=[], id=chatcmpl-DB6yyg8JWUautoLd4rCeuXk3mrHDp}], chatGenerationMetadata=DefaultChatGenerationMetadata[finishReason='STOP', filters=0, metadata=0]]]], context={}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sure! Hereâ€™s a simple **Hello World** program written in Kotlin:\n",
       "\n",
       "```kotlin\n",
       "fun main() {\n",
       "    println(\"Hello, World!\")\n",
       "}\n",
       "```\n",
       "\n",
       "When you run this program, it will print:\n",
       "\n",
       "```\n",
       "Hello, World!\n",
       "```"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
