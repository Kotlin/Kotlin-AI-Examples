{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Intro\n",
    "\n",
    "LLMs are becoming increasingly integrated into development workflows,\n",
    "creating demand for frameworks that make AI-based application development accessible.\n",
    "In the JVM and Kotlin ecosystem, Spring is exactly such a framework.\n",
    "Yes, that same Spring that has been around for over 20 years!\n",
    "\n",
    "Spring AI is a new addition that simplifies AI integration into your Kotlin code.\n",
    "Spring AI provides:\n",
    "\n",
    "* A unified API for working with various AI models and LLM providers (OpenAI, Anthropic, Ollama, etc.)\n",
    "* Components for prompt processing and context management\n",
    "* Built-in capabilities for vector stores and RAG applications\n",
    "\n",
    "In this series of posts, we'll explore the core features of Spring AI.\n",
    "Following programming tradition,\n",
    "our first post will be a \"Hello, world!\" of sorts:\n",
    "we'll connect to an LLM provider and ask it to tell us a joke.\n",
    "\n",
    "First, let's add the necessary dependencies.\n",
    "> [!NOTE] We'll be working with Claude models from Anthropic,\n",
    "> but we'll include commented code for OpenAI as well.\n",
    "> This will help you understand the general API approach regardless of which model you choose."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:33.694732Z",
     "start_time": "2026-02-19T22:14:33.202648Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_3_jupyter",
      "Line_4_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "%useLatestDescriptors\n",
    "%use spring-ai-anthropic\n",
    "//%use spring-ai-openai"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To use the model, we need to provide an API key.\n",
    "\n",
    "You can obtain this API key from\n",
    "[console.anthropic.com](https://console.anthropic.com/settings/keys)\n",
    "for Anthropic models or from\n",
    "[platform.openai.com](https://platform.openai.com/api-keys)\n",
    "for OpenAI models.\n",
    "\n",
    "Then add the generated API key to your environment variables:\n",
    "\n",
    "[MacOS/Linux]\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "export OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "\n",
    "```\n",
    "\n",
    "[Windows]\n",
    "```shell\n",
    "set ANTHROPIC_API_KEY=<INSERT KEY HERE> # for Anthropic\n",
    "set OPENAI_API_KEY=<INSERT KEY HERE> # for OpenAI\n",
    "```\n",
    "\n",
    "Let's retrieve the API key from environment variables:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:33.777472Z",
     "start_time": "2026-02-19T22:14:33.695443Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_5_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val apiKey = System.getenv(\"ANTHROPIC_API_KEY\") ?: \"YOUR_ANTHROPIC_API_KEY\"\n",
    "//val apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR_OPENAI_API_KEY\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, let's create a low-level API client for our chosen provider.\n",
    "This only requires the API key:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:34.224413Z",
     "start_time": "2026-02-19T22:14:33.791580Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_6_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicApi = AnthropicApi.builder().apiKey(apiKey).build()\n",
    "//val openAiApi = OpenAiApi.builder().apiKey(apiKey).build()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have our client, we can make a simple model call:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:36.087201Z",
     "start_time": "2026-02-19T22:14:34.224910Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_7_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicMessage = AnthropicApi.AnthropicMessage(\n",
    "    listOf(AnthropicApi.ContentBlock(\"Tell me a joke\")), AnthropicApi.Role.USER\n",
    ")\n",
    "//val openAiMessage = OpenAiApi.ChatCompletionMessage(\"Tell me a joke\", OpenAiApi.ChatCompletionMessage.Role.USER)\n",
    "\n",
    "\n",
    "anthropicApi.chatCompletionEntity(\n",
    "    AnthropicApi.ChatCompletionRequest(\n",
    "        AnthropicApi.ChatModel.CLAUDE_SONNET_4_5.value,\n",
    "        listOf(anthropicMessage), null, 100, 0.8, false\n",
    "    )\n",
    ")\n",
    "//openAiApi.chatCompletionEntity(\n",
    "//    OpenAiApi.ChatCompletionRequest(listOf(openAiMessage), OpenAiApi.ChatModel.GPT_5_CHAT_LATEST.value, 0.8, false)\n",
    "//)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 OK OK,ChatCompletionResponse[id=msg_01Uq1pL3ESv4y748HVFHNimM, type=message, role=ASSISTANT, content=[ContentBlock[type=TEXT, source=null, text=Why don't scientists trust atoms?\n",
       "\n",
       "Because they make up everything! ðŸ˜„, index=null, id=null, name=null, input=null, toolUseId=null, content=null, signature=null, thinking=null, data=null, cacheControl=null, title=null, context=null, citations=null, fileId=null, filename=null]], model=claude-sonnet-4-5-20250929, stopReason=end_turn, stopSequence=null, usage=Usage[inputTokens=11, outputTokens=20, cacheCreationInputTokens=0, cacheReadInputTokens=0], container=null],[Date:\"Thu, 19 Feb 2026 22:14:35 GMT\", Content-Type:\"application/json\", Transfer-Encoding:\"chunked\", Connection:\"keep-alive\", anthropic-ratelimit-output-tokens-limit:\"4250000\", anthropic-ratelimit-output-tokens-remaining:\"4250000\", anthropic-ratelimit-output-tokens-reset:\"2026-02-19T22:14:35Z\", anthropic-ratelimit-input-tokens-limit:\"16500000\", anthropic-ratelimit-input-tokens-remaining:\"16500000\", anthropic-ratelimit-input-tokens-reset:\"2026-02-19T22:14:35Z\", anthropic-ratelimit-requests-limit:\"20000\", anthropic-ratelimit-requests-remaining:\"19998\", anthropic-ratelimit-requests-reset:\"2026-02-19T22:14:34Z\", anthropic-ratelimit-tokens-limit:\"20750000\", anthropic-ratelimit-tokens-remaining:\"20750000\", anthropic-ratelimit-tokens-reset:\"2026-02-19T22:14:35Z\", X-Robots-Tag:\"none\", request-id:\"req_011CYJCTeqyYBmGQ2FkzpoXa\", strict-transport-security:\"max-age=31536000; includeSubDomains; preload\", anthropic-organization-id:\"ea22ac71-22a7-43cc-9c6a-0a6a2c25768f\", Server:\"cloudflare\", x-envoy-upstream-service-time:\"1315\", Content-Security-Policy:\"default-src 'none'; frame-ancestors 'none'\", cf-cache-status:\"DYNAMIC\", CF-RAY:\"9d091e315f84e50d-TXL\"]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Congratulations, that's our first joke!\n",
    "\n",
    "As you've noticed, this API doesn't look very convenient,\n",
    "but that's the nature of low-level APIs.\n",
    "\n",
    "Let's raise the abstraction level.\n",
    "We'll create `*ChatOptions` to help us set and use parameters for our LLM provider requests:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:36.129983Z",
     "start_time": "2026-02-19T22:14:36.088133Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_8_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicOptions = AnthropicChatOptions.builder()\n",
    "    .model(AnthropicApi.ChatModel.CLAUDE_SONNET_4_5)\n",
    "    .temperature(0.7)\n",
    "    .maxTokens(1024)\n",
    "    .build()\n",
    "//val openAiOptions = OpenAiChatOptions.builder()\n",
    "//    .model(OpenAiApi.ChatModel.GPT_5_CHAT_LATEST)\n",
    "//    .temperature(0.7)\n",
    "//    .build()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's create a `*ChatModel`, which will help us get another joke:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:36.167296Z",
     "start_time": "2026-02-19T22:14:36.130515Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_9_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val anthropicChat = AnthropicChatModel.builder()\n",
    "    .anthropicApi(anthropicApi)\n",
    "    .defaultOptions(anthropicOptions)\n",
    "    .build()\n",
    "//val openAiChat = OpenAiChatModel.builder()\n",
    "//    .openAiApi(openAiApi)\n",
    "//    .defaultOptions(openAiOptions)\n",
    "//    .build()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's ask for a joke about Kotlin:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:39.412650Z",
     "start_time": "2026-02-19T22:14:36.167769Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_10_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "anthropicChat.call(\"Tell me a joke about Kotlin\")\n",
    "//openAiChat.call(\"Tell me a joke about Kotlin\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Why did the Kotlin developer break up with Java?\n",
       "\n",
       "Because they got tired of all the `NullPointerException`s in the relationship! They wanted someone who was more null-safe. ðŸ˜„\n",
       "\n",
       "---\n",
       "\n",
       "*Bonus: In Kotlin, you can just add a `?` and move on with your life!*"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now we have a joke about Kotlin!\n",
    "\n",
    "This API is still specific to a particular provider.\n",
    "You might need an even higher level of abstraction,\n",
    "where your logic doesn't depend on which model you're using.\n",
    "\n",
    "For this, we'll use the `ChatClient`:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T22:14:41.454280Z",
     "start_time": "2026-02-19T22:14:39.421687Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_11_jupyter"
     ]
    }
   },
   "cell_type": "code",
   "source": [
    "val chatClient = ChatClient.create(anthropicChat)\n",
    "chatClient.prompt(\"Tell me a joke about Kotlin\").call().content()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Why do Kotlin developers sleep so well?\n",
       "\n",
       "Because they don't have to deal with NullPointerExceptions keeping them up at night!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Congratulations! Now we can write a simple joke and anecdote application.\n",
    "\n",
    "Check out the next notebooks to learn more about Kotlin and Spring AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
