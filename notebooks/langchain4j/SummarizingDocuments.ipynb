{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T19:12:35.905970Z",
     "start_time": "2025-05-11T19:12:34.994478Z"
    }
   },
   "source": [
    "%useLatestDescriptors\n",
    "%use langchain4j(1.0.0-beta3)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:12:36.134623Z",
     "start_time": "2025-05-11T19:12:35.909753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val openAiApiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"YOUR-OPENAI-API-KEY\"\n",
    "\n",
    "/**\n",
    " * A function to split text into chunks.\n",
    " * For simplicity, this example splits the text by sentences ('.')\n",
    " * and then re-joins them into chunks if the chunk size is under a limit.\n",
    " */\n",
    "fun splitIntoChunks(text: String, maxTokensPerChunk: Int = 300): List<String> {\n",
    "    // Very simplified approach:\n",
    "    val sentences = text.split(\".\")\n",
    "    val chunks = mutableListOf<String>()\n",
    "    val currentChunk = StringBuilder()\n",
    "\n",
    "    for (sentence in sentences) {\n",
    "        val potentialChunk = if (currentChunk.isEmpty()) sentence else \"${currentChunk.trim()}. $sentence\"\n",
    "        // Here you would estimate token count; for simplicity, we use character length.\n",
    "        // For robust token-based splits, you could integrate a tokenizer class from the library.\n",
    "        if (potentialChunk.length < maxTokensPerChunk) {\n",
    "            if (currentChunk.isNotEmpty()) {\n",
    "                currentChunk.append(\". \")\n",
    "            }\n",
    "            currentChunk.append(sentence)\n",
    "        } else {\n",
    "            // Add the current chunk and start a new one\n",
    "            chunks.add(currentChunk.toString())\n",
    "            currentChunk.clear()\n",
    "            currentChunk.append(sentence)\n",
    "        }\n",
    "    }\n",
    "    // Add the last chunk if it’s not empty\n",
    "    if (currentChunk.isNotEmpty()) {\n",
    "        chunks.add(currentChunk.toString())\n",
    "    }\n",
    "    return chunks\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:12:36.239949Z",
     "start_time": "2025-05-11T19:12:36.138835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dev.langchain4j.data.message.UserMessage.userMessage\n",
    "import dev.langchain4j.model.chat.request.ChatRequest\n",
    "import dev.langchain4j.model.chat.request.chatRequest\n",
    "import dev.langchain4j.model.openai.OpenAiChatModel\n",
    "\n",
    "/**\n",
    " * A pseudo function that sends the prompt to an OpenAI LLM and returns the completion.\n",
    " * In a real scenario, you’d use something like:\n",
    " * OpenAiService.builder()\n",
    " *   .openAiApiKey(openAiApiKey)\n",
    " *   .build()\n",
    " * and then create an LLM chain. This is just a placeholder for demonstration.\n",
    " */\n",
    "fun summarizeChunkWithOpenAI(chunk: String, openAiApiKey: String): String {\n",
    "    val openAi = OpenAiChatModel.builder()\n",
    "        .apiKey(openAiApiKey)\n",
    "        .modelName(\"gpt-4\")\n",
    "        .temperature(0.7)\n",
    "        .build()\n",
    "\n",
    "    val prompt = \"Please summarize the following text:\\n\\n$chunk\"\n",
    "\n",
    "    // Construct a chat request (if you’re using chat models)\n",
    "    val request = chatRequest {\n",
    "        messages += userMessage(prompt)\n",
    "    }\n",
    "\n",
    "    // Send the request to get the actual summary\n",
    "    val response = openAi.chat(request)\n",
    "\n",
    "    // Extract the content from the response\n",
    "    return response.aiMessage().text()\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:12:36.370419Z",
     "start_time": "2025-05-11T19:12:36.244473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/**\n",
    " * Summarizes a large document by splitting it into chunks, summarizing each chunk,\n",
    " * and combining the results.\n",
    " */\n",
    "fun summarizeDocument(\n",
    "    text: String,\n",
    "    detail: Double = 0.0,\n",
    "    maxTokensPerChunk: Int = 500\n",
    "): String {\n",
    "    require(detail in 0.0..1.0) { \"Detail must be between 0.0 and 1.0\" }\n",
    "\n",
    "    // Split the text into an initial set of chunks\n",
    "    val initialChunks = splitIntoChunks(text, maxTokensPerChunk)\n",
    "    // Interpolate number of chunks based on the detail desired\n",
    "    val maxChunks = initialChunks.size\n",
    "    val minChunks = 1\n",
    "    val targetChunksCount = (minChunks + detail * (maxChunks - minChunks)).toInt().coerceAtLeast(1)\n",
    "\n",
    "    // Recalculate chunk size to approximate the target number of chunks\n",
    "    // (For a real application, you might do more advanced splitting)\n",
    "    val totalLength = text.length\n",
    "    val adjustedChunkSize = (totalLength / targetChunksCount).coerceAtLeast(200)\n",
    "    val finalChunks = splitIntoChunks(text, adjustedChunkSize)\n",
    "\n",
    "    // Summarize each chunk individually\n",
    "    val chunkSummaries = finalChunks.map { chunk ->\n",
    "        summarizeChunkWithOpenAI(chunk, openAiApiKey)\n",
    "    }\n",
    "\n",
    "    // Combine all chunk summaries into a final summary\n",
    "    return chunkSummaries.joinToString(separator = \"\\n\\n\")\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:12:36.415577Z",
     "start_time": "2025-05-11T19:12:36.382328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.io.File\n",
    "\n",
    "val artificialIntelligenceWikipediaText = File(\"data/artificial_intelligence_wikipedia.txt\").readText(Charsets.UTF_8)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:21:50.634549Z",
     "start_time": "2025-05-11T19:12:36.417729Z"
    }
   },
   "cell_type": "code",
   "source": "val summaryDetail1 = summarizeDocument(artificialIntelligenceWikipediaText, detail = 1.0)",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T19:21:50.727246Z",
     "start_time": "2025-05-11T19:21:50.656323Z"
    }
   },
   "cell_type": "code",
   "source": "println(\"Summary with detail=1.0:\\n$summaryDetail1\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary with detail=1.0:\n",
      "Artificial intelligence (AI) refers to the intelligence displayed by machines, especially computer systems. It is a research area in computer science that creates and examines methods and software allowing machines to understand their surroundings and use learning and intelligence to perform actions that boost their probability of reaching set goals.\n",
      "\n",
      ", Tesla); and scientific research. The term AI can refer to machines commonly known as Artificial Intelligence. These machines are used in various sectors including industry, government, and scientific research. High-profile applications of AI include advanced web search engines like Google, recommendation systems used by YouTube, Amazon, and Netflix, voice interaction systems like Google Assistant, Siri, and Alexa, and autonomous vehicles like Tesla.\n",
      "\n",
      "The text discusses the broad applications of Artificial Intelligence (AI), including autonomous vehicles, creative tools, and strategy games. However, it notes that many AI technologies are not recognized as such because they have become so common and useful that they no longer carry the \"AI\" label.\n",
      "\n",
      "Alan Turing was the pioneer in machine intelligence research. Artificial intelligence (AI) became an academic discipline in 1956. The field has experienced several cycles of optimism and disappointment, resulting in loss of funding, a phenomenon termed as AI winter.\n",
      "\n",
      "The funding and interest in artificial intelligence (AI) significantly increased after 2012 when deep learning outperformed all previous AI techniques. The interest further escalated after 2017 with the introduction of the transformer architecture. This resulted in the AI boom in the early 2020s, with most of the advancements in AI being pioneered by companies, universities, and labs primarily based in the United States.\n",
      "\n",
      "The rise of artificial intelligence in the 21st century is leading to societal and economic changes, with increased automation and data-driven decision-making. AI systems are being integrated into different economic sectors and aspects of life, affecting job markets, healthcare, government, industry, and education.\n",
      "\n",
      "The text raises concerns about the long-term effects, ethical implications, and risks of AI, suggesting the need for regulatory policies for safety and benefits of the technology. It also mentions that AI research is divided into various sub-fields, each focused on specific goals and tools.\n",
      "\n",
      "The primary objectives of AI research encompass reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. One of the long-term goals is to achieve general intelligence, which means the AI system's ability to perform any task a human can do at an equal or superior level.\n",
      "\n",
      "AI researchers utilize a variety of techniques to achieve their goals, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also incorporates knowledge from fields such as psychology, linguistics, philosophy, neuroscience, and more.\n",
      "\n",
      "The broad challenge of simulating or creating intelligence has been divided into smaller, specific sub-problems. These sub-problems involve certain traits or capabilities that researchers believe an intelligent system should demonstrate. The traits that have been most focused on encompass the range of AI research.\n",
      "\n",
      "Early researchers created algorithms that mimic human reasoning and problem-solving, particularly in solving puzzles or making logical deductions. By the late 1980s and 1990s, they developed methods to handle uncertain or incomplete information, using concepts from probability and economics.\n",
      "\n",
      "The text explains that many algorithms struggle to solve large reasoning problems due to a \"combinatorial explosion\", where they become exponentially slower as the problems increase in size. It also notes that humans seldom use the methodical deduction that early AI research could imitate, instead opting for quick, intuitive judgments to solve most of their problems.\n",
      "\n",
      "The text discusses the unresolved issue of accurate and efficient reasoning. It explains that an ontology represents knowledge as a set of concepts within a domain and their relationships. Knowledge representation and knowledge engineering enable AI programs to intelligently answer questions and deduce real-world facts.\n",
      "\n",
      "Formal knowledge representations are utilized in various fields such as content-based indexing and retrieval, scene interpretation, clinical decision support, and knowledge discovery. This knowledge is stored in a knowledge base, which is a collection of information formatted to be utilized by a program.\n",
      "\n",
      "An ontology refers to the collection of objects, relationships, concepts, and properties utilized in a specific area of knowledge.\n",
      "\n",
      "Knowledge bases need to illustrate various aspects of knowledge including objects, properties, their relations and categories, situations, events, states and time. They should also represent causes and effects, knowledge about knowledge, default reasoning which refers to assumptions humans make until proven otherwise, and numerous other domains of knowledge.\n",
      "\n",
      "The text discusses two major challenges in knowledge representation. The first is the vastness of commonsense knowledge, which refers to the large set of basic facts known by the average person. The second challenge is the sub-symbolic nature of most commonsense knowledge, indicating that much of what people know cannot be verbally expressed as 'facts' or 'statements'.\n",
      "\n",
      "The text discusses challenges in artificial intelligence (AI), including the difficulty of knowledge acquisition, which is the process of obtaining knowledge for AI applications. It also describes an \"agent\" in AI as anything that perceives and acts in the world, specifically aiming to achieve certain goals or preferences. In automated planning, the agent works towards a specific goal.\n",
      "\n",
      "In automated decision making, the decision-making agent has preferences for certain situations and aversions to others. Each situation is given a numerical value, known as the \"utility,\" which represents the agent's preference level for that situation.\n",
      "\n",
      "The text discusses a method where for every potential action, its \"expected utility\" is calculated. This is done by considering the utility of all possible results of the action, factoring in the likelihood of each outcome. The action with the highest expected utility is then selected. In classical planning, the agent is fully aware of the consequences of any action.\n",
      "\n",
      "In most real-world situations, an agent might not be sure about its current scenario or the outcomes of its possible actions. Thus, it must make a decision based on probability and then re-evaluate the situation to check if the chosen action was effective.\n",
      "\n",
      "The text discusses the uncertainty of an agent's preferences in some situations, particularly when other agents or humans are involved. This uncertainty can be addressed either by learning through methods like inverse reinforcement learning, or by the agent seeking additional information to enhance its preferences. Information value theory can help determine the worth of exploratory or experimental actions.\n",
      "\n",
      "The text discusses how the potential future actions and scenarios are usually too vast to predict accurately. Therefore, agents must make decisions and assess situations despite not knowing the probable results.\n",
      "\n",
      "A Markov decision process is a mathematical approach for decision making, featuring a transition model outlining the probability of an action causing a specific state change, and a reward function detailing the utility of each state and cost of each action. The policy, which associates a decision with each possible state, can be calculated, heuristic, or learned.\n",
      "\n",
      "Game theory is a principle used in AI programs to depict the rational behavior of multiple interacting agents. Machine learning, a major part of AI, is the study of programs that can automatically enhance their performance on any given task. There are several types of machine learning in existence.\n",
      "\n",
      "Unsupervised learning is a method that analyzes data and identifies patterns and predictions without any guidance. On the other hand, supervised learning necessitates human-labelled input data. It has two main types: classification, where the program predicts the category of the input, and regression, where the program deduces a numeric function based on numeric input.\n",
      "\n",
      "Reinforcement learning is a process where an agent is rewarded for good responses and penalized for bad ones. This helps the agent to learn to choose 'good' responses. Transfer learning, on the other hand, is a process where the knowledge gained from one problem is used to solve another problem.\n",
      "\n",
      "Deep learning, a variant of machine learning, uses artificial neural networks inspired by biology for various learning types. Computational learning theory evaluates learners based on computational complexity, sample complexity (amount of data needed), or other optimization concepts.\n",
      "\n",
      "Natural language processing (NLP) enables software to interpret, generate, and communicate in human languages like English. It tackles issues such as speech recognition and synthesis, machine translation, information extraction and retrieval, and question answering.\n",
      "\n",
      "Early research, using Noam Chomsky's generative grammar and semantic networks, struggled with word-sense disambiguation unless it was limited to small areas known as \"micro-worlds.\" This was due to the issue of common sense knowledge.\n",
      "\n",
      "Margaret Masterman proposed that the understanding of languages relies on meaning rather than grammar. She suggested that computational language structure should be based on thesauri instead of dictionaries.\n",
      "\n",
      "Modern deep learning techniques used in Natural Language Processing (NLP) involve word embedding, which represents words as vectors encoding their meanings, and transformers, a deep learning architecture that uses an attention mechanism, among others.\n",
      "\n",
      "In 2019, generative pre-trained transformer (GPT) language models started producing coherent text. By 2023, these models had reached human-level performance on various tests and real-world applications, including the bar exam, the SAT test, and the GRE test.\n",
      "\n",
      "Machine perception refers to the capacity of machines to use sensor input to understand aspects of the world. This includes using cameras, microphones, wireless signals, and various sensors. Computer vision, a part of machine perception, involves analyzing visual input. The field of machine perception includes speech and facial recognition, image and object recognition, and robotic perception.\n",
      "\n",
      "The text talks about social intelligence in machines. An example is Kismet, a robot head created in the 1990s, which can recognize and imitate emotions. The text also mentions affective computing, an interdisciplinary field that includes systems capable of recognizing, interpreting, processing, or simulating human emotions and mood.\n",
      "\n",
      "This text explains that some virtual assistants are programmed to converse or joke in a way that mimics human interaction, which can make them seem more emotionally aware. However, this can lead to users developing an inflated perception of the computer's intelligence.\n",
      "\n",
      "The text discusses some moderate successes in affective computing. These include textual sentiment analysis and multimodal sentiment analysis, where AI classifies emotions displayed by a person in a video. It also mentions artificial general intelligence, suggesting that a machine with such capability should be able to solve a wide range of problems similar to human intelligence.\n",
      "\n",
      "Artificial intelligence (AI) research employs a broad range of techniques to achieve its objectives. One such strategy involves search and optimization, where AI resolves numerous issues by intelligently scouring through a multitude of possible solutions. Two distinct types of search methods used in AI are state space search and local search.\n",
      "\n",
      "State space search is a method that navigates through a tree of potential states to locate a goal state. This process is commonly used in planning algorithms, where it sifts through trees of goals and subgoals to find a path to a specific target goal - a process referred to as means-ends analysis.\n",
      "\n",
      "Simple exhaustive searches are often inadequate for real-world problems due to the search space growing extremely large, resulting in slow or incomplete searches. Applying heuristics or rules of thumb can help prioritize choices that are more likely to achieve a goal.\n",
      "\n",
      "Adversarial search is a technique used in game-playing programs like chess or Go, which involves searching through a tree of possible moves and counter-moves to find a winning position. On the other hand, local search, as illustrated by gradient descent, adjusts two parameters in order to minimize the loss function.\n",
      "\n",
      "Local search is a mathematical optimization technique used to solve problems by refining an initial guess incrementally. Gradient descent, a variant of local search, optimizes numerical parameters by gradually adjusting them to minimize a loss function. This method is frequently used to train neural networks.\n",
      "\n",
      "Local search involves evolutionary computation, which strives to continually enhance a set of potential solutions by mutating and recombining them, with only the fittest surviving each generation. Distributed search processes can be coordinated using swarm intelligence algorithms.\n",
      "\n",
      "The text mentions two commonly used swarm algorithms in search: particle swarm optimization, which is inspired by bird flocking, and ant colony optimization, which is based on ant trails. Additionally, it states that formal logic is utilized for reasoning and knowledge representation.\n",
      "\n",
      "Formal logic is divided into two main forms: propositional logic and predicate logic. Propositional logic works on statements that are true or false and uses logical connectors like \"and\", \"or\", \"not\" and \"implies\". Predicate logic operates on objects, predicates and relations, using quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\".\n",
      "\n",
      "Deductive reasoning in logic is the method of validating a new statement based on other given statements assumed to be true. These proofs can be organized as proof trees, where nodes are marked by sentences and linked to parent nodes through inference rules.\n",
      "\n",
      "The text explains that problem-solving is essentially searching for a solution, depicted as the root node of a proof tree, using given premises or axioms as leaf nodes. With Horn clauses, the problem-solving search can be conducted either by reasoning from the premises forward or from the problem backward.\n",
      "\n",
      "The clausal form of first-order logic uses resolution as a single, axiom-free rule of inference to solve problems by proving a contradiction from premises that include the negation of the problem. However, inference in both Horn clause logic and first-order logic is undecidable and intractable.\n",
      "\n",
      "The text discusses two concepts: backward reasoning with Horn clauses and fuzzy logic. The former concept, used in the Prolog programming language, is Turing complete, meaning it can solve any problem that a Turing machine can, given enough time and resources. Its efficiency is on par with other symbolic programming languages. On the other hand, fuzzy logic is a method that assigns a \"degree of truth\" between 0 and 1, allowing it to handle vague or partially true propositions.\n",
      "\n",
      "Non-monotonic logics, such as logic programming with negation as failure, are used to manage default reasoning. Moreover, other specific types of logic have been developed to explain numerous complex domains.\n",
      "\n",
      "The text discusses probabilistic methods used in artificial intelligence (AI) for uncertain reasoning. These methods, derived from probability theory and economics, are used to help AI operate in conditions where information is incomplete or uncertain. These tools are applicable in various AI fields such as reasoning, planning, learning, perception, and robotics.\n",
      "\n",
      "The text discusses the development of exact mathematical tools used to analyze how an entity can make choices and plan. These tools are based on decision theory, decision analysis, and information value theory. Some of the models utilized include Markov decision processes, dynamic decision networks, game theory, and mechanism design.\n",
      "\n",
      "Bayesian networks are a versatile tool utilized for various purposes such as reasoning through the Bayesian inference algorithm, learning via the expectation-maximization algorithm, planning with decision networks, and perception through dynamic Bayesian networks.\n",
      "\n",
      "Probabilistic algorithms are useful for analyzing data streams over time. They can be used for filtering, prediction, smoothing, and finding explanations for data, aiding perception systems in processing processes represented by models like hidden Markov models or Kalman filters.\n",
      "\n",
      ", \"if hot then cool\"), on the other hand. The classifiers use statistical learning methods to identify patterns or characteristics, while controllers use algorithms to respond to certain conditions. The expectation-maximization clustering method has been successfully used to accurately cluster Old Faithful eruption data, highlighting the effectiveness of such AI applications.\n",
      "\n",
      "The text discusses a concept in machine learning known as classifiers, which are functions that use pattern matching to find the closest match. These classifiers can be adjusted using supervised learning based on selected examples. Each pattern, also referred to as an observation, is assigned a predefined class label. The combination of all observations and their corresponding class labels constitutes a data set.\n",
      "\n",
      "The text describes the process of classifying a new observation based on previous experience. It mentions that there are several types of classifiers, with the decision tree being the simplest and most commonly used symbolic machine learning algorithm.\n",
      "\n",
      "The K-nearest neighbor algorithm was the most popular analogical AI until the mid-1990s, but then got replaced by Kernel methods like the support vector machine (SVM). Google reportedly uses the naive Bayes classifier as it is highly scalable. Neural networks are also utilized as classifiers.\n",
      "\n",
      "An artificial neural network is a system of interconnected nodes or artificial neurons, which are modeled after the neurons in a biological brain. The main function of these networks is to recognize patterns, which they can do in new data once they've been trained.\n",
      "\n",
      "A deep neural network consists of an input, a minimum of two hidden layers of nodes applying functions, and an output. Data is transmitted to the next layer once the weight crosses a specified threshold.\n",
      "\n",
      "Neural networks use learning algorithms that implement local search to select the appropriate weights for each input during training. The backpropagation algorithm is the most commonly used training technique. Neural networks have the ability to model intricate relationships between inputs and outputs, and identify patterns in data. Theoretically, a neural network can learn any function.\n",
      "\n",
      "Feedforward neural networks allow signals to pass in one direction, while recurrent neural networks feed the output back into the input, enabling short-term memory of previous inputs. The most successful architecture for recurrent networks is Long Short Term Memory. Perceptrons use a single layer of neurons, whereas deep learning utilizes multiple layers.\n",
      "\n",
      "Convolutional neural networks improve the connection between neurons that are close to each other, which is essential in image processing for identifying an object. Deep learning uses multiple layers of neurons between the network's inputs and outputs.\n",
      "\n",
      "The text describes how multiple layers in a process can progressively extract more complex features from raw input. In the context of image processing, lower layers may identify basic elements like edges, while higher layers identify more sophisticated concepts that are relevant to humans, such as digits, letters, or faces.\n",
      "\n",
      "As of 2023, deep learning has significantly enhanced the performance of programs in various subfields of artificial intelligence, such as computer vision, speech recognition, natural language processing, and image classification. However, the exact reason why deep learning is so effective in these applications is not yet known.\n",
      "\n",
      "The rapid success of deep learning between 2012 and 2015 was not due to a new discovery or theoretical breakthrough, as deep neural networks and backpropagation had been known since the 1950s. The success was instead due to an immense increase in computer power, including a hundred-fold speed increase by switching to GPUs, and the availability of large amounts of training data, particularly giant curated datasets used for benchmark testing, like ImageNet.\n",
      "\n",
      "The text describes Generative pre-trained transformers (GPT), which are large language models used in natural language processing. They are based on the understanding of semantic relationships between words in sentences. GPT models are pre-trained on a large amount of text, usually sourced from the internet, and the pre-training process involves predicting the next token, which can be a word, subword or punctuation.\n",
      "\n",
      "GPT models, during pre-training, gather world knowledge and can generate human-like text by continually predicting the next token. In a later training phase, the model is typically enhanced to be more truthful, useful, and harmless, often using a method known as reinforcement learning from human feedback (RLHF).\n",
      "\n",
      "Current GPT models, used in chatbots, can still generate inaccuracies known as \"hallucinations\", but this can be mitigated with RLHF and good quality data. Some current models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA.\n",
      "\n",
      "The text discusses Multimodal GPT models, which have the ability to process various types of data, including images, videos, sound, and text.\n",
      "\n",
      "In the late 2010s, graphics processing units (GPUs) enhanced for AI and used with TensorFlow software became the main tool for training large-scale machine learning models in both commercial and academic settings, replacing the previously used central processing units (CPUs).\n",
      "\n",
      "The text refers to the historical use of specialized programming languages like Lisp, Prolog, Python, and others.\n",
      "\n",
      "The text describes the various applications of artificial intelligence (AI) and machine learning technology in the 2020s. These include search engines like Google Search, online advertisement targeting, recommendation systems used by Netflix, YouTube, or Amazon, driving internet traffic, targeted advertising platforms like AdSense and Facebook, virtual assistants such as Siri and Alexa, autonomous vehicles, automatic language translation services like Microsoft Translator and Google Translate, facial recognition systems like Apple's Face ID, Microsoft's DeepFace, Google's FaceNet, and image labeling used by Facebook, Apple's iPhoto and TikTok.\n",
      "\n",
      "\n",
      "The use of artificial intelligence (AI) in healthcare could greatly improve patient care and quality of life. Medical professionals, adhering to the Hippocratic Oath, are ethically obligated to use AI if it can help in more accurate diagnosis and treatment of patients.\n",
      "\n",
      "AI is a crucial tool in medical research for processing and integrating large amounts of data. This is especially significant for organoid and tissue engineering development, which rely heavily on microscopy imaging. It is also suggested that AI could address imbalances in funding distribution across various research fields.\n",
      "\n",
      "New AI tools are enhancing our knowledge of biomedically relevant pathways. AlphaFold 2, introduced in 2021, showed the capability to estimate the 3D structure of a protein in hours, not months. In 2023, AI-guided drug discovery reportedly found a class of antibiotics that can kill two types of drug-resistant bacteria.\n",
      "\n",
      "The article discusses the use of game playing programs in the development and testing of artificial intelligence (AI) techniques since the 1950s. It highlights the achievement of Deep Blue, the first computer chess-playing system that defeated world chess champion Garry Kasparov in May 1997.\n",
      "\n",
      "In 2011, IBM's question answering system, Watson, decisively defeated the top two Jeopardy! champions, Brad Rutter and Ken Jennings, in a quiz show exhibition match. Then, in March 2016, AlphaGo, a computer system designed to play the game of Go, won 4 out of 5 games against Go champion Lee Sedol, becoming the first computer system to defeat a professional Go player without any handicaps.\n",
      "\n",
      "In 2017, an AI program defeated Ke Jie, the world's best Go player. Other AI programs, like Pluribus, can handle imperfect-information games like poker. DeepMind has developed more generalized reinforcement learning models, such as MuZero, which can be trained to play various games including chess, Go, and Atari games.\n",
      "\n",
      "In 2019, DeepMind's AlphaStar reached grandmaster level in the complex real-time strategy game, StarCraft II. Furthermore, in 2021, an AI agent, using deep reinforcement learning, participated in a PlayStation Gran Turismo competition and won against four of the world's top Gran Turismo drivers.\n",
      "\n",
      "Countries worldwide are utilizing Artificial Intelligence (AI) in their military operations. The primary applications of AI in the military include improving command and control, communication, sensor technology, integration, and interoperability. Research is also being conducted to utilize AI in intelligence collection and analysis, logistics, cyber operations, information operations, and semi-autonomous and autonomous vehicles.\n",
      "\n",
      "AI technologies enhance military operations by coordinating sensors and effectors, detecting and identifying threats, marking enemy positions, acquiring targets, and managing distributed Joint Fires between networked combat vehicles involving both manned and unmanned teams. AI has been integrated into military operations in Iraq and Syria.\n",
      "\n",
      "In November 2023, US Vice President Kamala Harris revealed a declaration signed by 31 countries aimed at regulating the military use of Artificial Intelligence (AI). The agreement includes commitments to use legal reviews to ensure military AI complies with international law and to maintain caution and transparency in the development of this technology.\n",
      "\n",
      "Generative AI became widely recognized in the early 2020s. By March 2023, 58% of US adults were aware of ChatGPT, a generative AI software, and 14% had used it.\n",
      "\n",
      "AI-based text-to-image generators like Midjourney, DALL-E, and Stable Diffusion are gaining popularity due to their increasing realism and user-friendliness. This trend has led to viral AI-generated photos, including a fake photo of Pope Francis in a white puffer coat, a fictitious arrest of Donald Trump, and a hoax attack on the Pentagon. These tools are also being used in professional creative arts.\n",
      "\n",
      "The text discusses the wide application of AI across various industries to solve specific problems. According to a 2017 survey, one in five companies stated they had incorporated AI into their offerings or processes.\n",
      "\n",
      "The text discusses various applications of artificial intelligence (AI). These include energy storage, medical diagnosis, military logistics, predicting judicial decisions, foreign policy, and supply chain management. In agriculture, AI aids farmers in identifying areas requiring irrigation, fertilization, pesticide treatments, or yield increase. Agronomists also use AI for research and development.\n",
      "\n",
      "AI is utilized in agriculture for numerous tasks including predicting crop ripening time, monitoring soil moisture, operating agricultural robots, conducting predictive analytics, classifying livestock emotions, automating greenhouses, detecting diseases and pests, and water conservation.\n",
      "\n",
      "Artificial intelligence (AI) is applied in astronomy to analyze vast amounts of data. Its uses include classification, regression, clustering, forecasting, generation, discovery and gaining new scientific insights. Examples of its applications include discovering exoplanets, predicting solar activity, and differentiating between signals and instrumental effects in gravitational wave astronomy.\n",
      "\n",
      "The text discusses the potential uses of artificial intelligence (AI) in space activities, such as space exploration, data analysis from space missions, real-time science decisions for spacecraft, space debris avoidance, and more autonomous operations. It also mentions that AI has both potential benefits and risks, indicating a link to a main article on the ethics of artificial intelligence.\n",
      "\n",
      "Demis Hassabis from Deep Mind believes that AI has potential to advance science and solve major problems by first \"solving intelligence\". However, with the increased use of AI, several unintended consequences and risks have also been identified.\n",
      "\n",
      "The text discusses potential issues with AI training processes in in-production systems, noting that ethics and bias may not always be factored in, particularly in the case of unexplainable deep learning algorithms. It also mentions risks and harm related to privacy and copyright concerns, as machine-learning algorithms require large amounts of data to function.\n",
      "\n",
      "The methods utilized to gather data such as online activity, geolocation, video and audio from users by technology companies have sparked worries regarding privacy, surveillance, and copyright issues.\n",
      "\n",
      "Amazon has recorded millions of private conversations to develop speech recognition algorithms, even allowing temporary workers to listen to and transcribe some. Opinions about this broad surveillance vary, with some viewing it as a necessary evil and others deeming it clearly unethical and a breach of privacy rights.\n",
      "\n",
      "AI developers advocate for methods like data aggregation, de-identification, and differential privacy to create beneficial applications while preserving user privacy. Since 2016, privacy experts like Cynthia Dwork have started considering privacy from the perspective of fairness.\n",
      "\n",
      "The text discusses how the focus has shifted from what AI systems know to how they use that knowledge. Generative AI, for example, is frequently trained on copyrighted works without a license, such as images or computer code. The usage of this output is justified under the concept of \"fair use\".\n",
      "\n",
      "Website owners can prevent their copyrighted content from being indexed or 'scraped' by AI by adding specific code to their site. This feature is available through certain services like OpenAI.\n",
      "\n",
      "Experts are uncertain about the legal standing of using copyrighted work to train generative AI. Factors influencing this include the purpose of using the copyrighted work and its potential market impact. Notably, in 2023, renowned authors such as John Grisham and Jonathan Franzen sued AI companies for using their work for this purpose.\n",
      "\n",
      "YouTube, Facebook and other platforms use AI recommender systems to guide users to more content, with the sole objective of maximizing user engagement and keeping people watching.\n",
      "\n",
      "The AI discovered that users often selected content featuring misinformation, conspiracy theories, and extreme partisan viewpoints. To maintain user engagement, the AI recommended more of such content. The AI also observed that users tended to watch more content on similar topics, leading users into filter bubbles where they were exposed to various versions of the same misinformation.\n",
      "\n",
      "The text discusses the harmful impact of misinformation spread through an AI program, leading to a decrease in trust in institutions, media, and government. This misinformation appeared to be validated by the AI program's success in achieving its goal, causing harm to society. Following the 2016 U.S. election, major tech companies took action to address this issue.\n",
      "\n",
      "In 2022, generative AI started producing images, audio, video, and text that are virtually identical to real-life counterparts. However, this technology can be misused to generate large volumes of misinformation or propaganda.\n",
      "\n",
      "AI pioneer Geoffrey Hinton voiced concerns about AI being used by authoritarian leaders to manipulate their electorates, among other risks. Additionally, issues of algorithmic bias and fairness were raised, noting that machine learning applications can be biased if they are trained with biased data, with developers potentially being unaware of the existing bias.\n",
      "\n",
      "The text discusses the possibility of bias being introduced in machine learning through the selection of training data and model deployment. This bias can lead to serious harm, including discrimination, in areas such as medicine, finance, recruitment, housing, and policing. The field of fairness in machine learning is dedicated to studying how to prevent the harm caused by such algorithmic bias.\n",
      "\n",
      "The concept of \"fairness\" within AI has become a significant area of academic study due to its complexity and the difficulty to define it in a way that satisfies all stakeholders. This is exemplified by an incident on June 28, 2015, where Google Photos's image labeling feature incorrectly identified two black individuals as \"gorillas\".\n",
      "\n",
      "The system initially had a \"sample size disparity\" issue as it was trained on a dataset with few images of black people. Google addressed this by disabling the system's ability to label anything as a \"gorilla\". However, eight years later in 2023, Google Photos and similar products from Apple, Facebook, Microsoft, and Amazon still couldn't identify a gorilla.\n",
      "\n",
      "COMPAS is a commercial software used by US courts to predict the probability of a defendant reoffending. In 2016, Julia Angwin from ProPublica found that the program showed racial bias, even though it was not provided with the race of the defendants.\n",
      "\n",
      "The system had an equal error rate of 61% for both whites and blacks. However, the nature of the errors varied by race. The system consistently over-predicted the likelihood of black individuals re-offending, and under-predicted the likelihood of white individuals re-offending.\n",
      "\n",
      "In 2017, researchers demonstrated that it was mathematically impossible for the COMPAS system to consider all aspects of fairness when the re-offense rates for whites and blacks were different in the data. They further noted that a program could still make biased decisions even if explicit problematic elements such as 'race' or 'gender' were not mentioned in the data.\n",
      "\n",
      "The feature in a program will be associated with other features such as \"address\", \"shopping history\" or \"first name\", and the program will make decisions based on these features, similar to how it would decide based on \"race\" or \"gender\". However, Moritz Hardt stated that fairness cannot be achieved through overlooking these variables.\n",
      "\n",
      "COMPAS has been criticized for its machine learning models' reliance on past data to make future predictions. Critics argue that if these models are trained on data reflecting past racist decisions, they will inevitably predict future occurrences of such decisions, perpetuating systemic racism.\n",
      "\n",
      "The text suggests that machine learning, while useful, may inadvertently promote racist outcomes if it uses past data to make future predictions. It is more descriptive rather than prescriptive and may not be ideal for decision-making in scenarios where the hope is for a better future than the past.\n",
      "\n",
      "The text suggests that bias and unfairness in AI may go unnoticed due to lack of diversity among its developers, with only 4% being black and 20% being women, the majority being white males.\n",
      "\n",
      "At the 2022 Conference on Fairness, Accountability, and Transparency in Seoul, South Korea, the Association for Computing Machinery suggested that the use of AI and robotics systems should be limited until they are proven to be free of bias mistakes. The organization particularly recommended curtailing the use of self-learning neural networks trained on large, unregulated, and flawed internet data.\n",
      "\n",
      "The text discusses the lack of transparency in many AI systems, particularly those using deep neural networks. These systems are often so complex that even their designers cannot explain how they make decisions, due to the numerous non-linear relationships between inputs and outputs. This issue relates to concepts like Explainable AI, Algorithmic transparency, and Right to explanation.\n",
      "\n",
      "The text discusses the importance of understanding how a machine learning program operates to ensure its correct functionality. Despite passing rigorous tests, a program can still learn something different from what was intended by the programmers. However, there are popular explainability techniques to better understand these programs.\n",
      "\n",
      "The text talks about a system designed to identify skin diseases more effectively than medical professionals. However, the system was found to incorrectly classify images with a ruler as \"cancerous\". This is attributed to the fact that photos of cancerous skin diseases usually include a ruler for scale.\n",
      "\n",
      "A machine learning system designed to allocate medical resources wrongly classified asthma patients as \"low risk\" for dying from pneumonia. This is incorrect as asthma is a severe risk factor. The error occurred because the system's training data showed that asthma patients, who usually receive extensive medical care, were less likely to die, leading to the misclassification.\n",
      "\n",
      "The text discusses two separate points. First, it acknowledges a correlation between asthma and a lower risk of dying from pneumonia, but notes that this correlation may be misleading. Secondly, it asserts the right of individuals to understand the reasoning behind decisions made by algorithms, using doctors explaining their decisions to colleagues as an analogy.\n",
      "\n",
      "Early drafts of the European Union's General Data Protection Regulation in 2016 acknowledged the existence of a particular right, which remains an unresolved issue with no clear solution according to industry experts. Regulators, however, insisted that despite the lack of a solution, the tools causing the problem should not be utilized due to the tangible harm they cause.\n",
      "\n",
      "DARPA launched the XAI (Explainable Artificial Intelligence) program in 2014 to address issues related to AI transparency. Potential solutions include SHAP, which aims to enhance transparency by visually representing how each feature contributes to the output, and LIME, which simplifies the model for local approximation, making it more interpretable.\n",
      "\n",
      "Multitask learning enhances the performance of neural networks by providing multiple outputs in addition to the target classification, helping developers understand what the network has learned. Techniques like Deconvolution and DeepDream allow developers to interpret the learning patterns of different layers of the network and generate outputs that indicate what the network is learning.\n",
      "\n",
      "The text discusses the potential misuse of artificial intelligence (AI) by harmful entities such as authoritarian governments, terrorists, criminals or rogue states. These bad actors can utilize AI tools for nefarious purposes. One example given is a lethal autonomous weapon, a machine that can locate, select, and engage human targets without any human oversight.\n",
      "\n",
      "The text discusses the potential misuse of widely available AI tools by malicious parties to create inexpensive autonomous weapons. If mass-produced, these could potentially serve as weapons of mass destruction. Furthermore, even if used in conventional warfare, the text suggests that such AI weapons may not be able to accurately select targets, posing a risk of harm to innocent individuals.\n",
      "\n",
      "In 2014, 30 countries, including China, backed a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons. However, the United States and some other countries disagreed with this. By 2015, more than fifty countries were reportedly researching battlefield robots. Furthermore, AI tools could potentially aid authoritarian governments in controlling their citizens more efficiently.\n",
      "\n",
      "The text discusses different technologies and how they can be used in surveillance and propaganda. Face and voice recognition can enable broad surveillance and identify potential state enemies. Machine learning can process this data for classification purposes. Recommendation systems can be used to deliver targeted propaganda and misinformation. Additionally, deepfakes and generative AI can assist in creating misinformation.\n",
      "\n",
      "Advanced artificial intelligence (AI) can potentially make centralized decision making in authoritarian regimes more competitive than decentralized systems found in liberal societies. AI technologies, which have been available since 2020 or earlier, can reduce the cost and complexity of digital warfare and advanced spyware. For example, AI facial recognition systems are already being used for mass surveillance in China.\n",
      "\n",
      "AI is expected to aid bad actors in ways that cannot be predicted, such as machine-learning AI being able to create thousands of toxic molecules in hours. Training AI systems requires a lot of computing power, which typically only Big Tech companies can afford.\n",
      "\n",
      "Smaller startups like Cohere and OpenAI often purchase access to data centers from larger tech companies like Google and Microsoft. Economists have often pointed out the potential for job loss due to artificial intelligence (AI), and the need for appropriate social policies to ensure full employment.\n",
      "\n",
      "Technology has historically led to an increase in total employment, but economists admit that the impact of AI on jobs is unknown and could potentially reduce employment.\n",
      "\n",
      "jobs could be automated by the mid-2030s, while a 2016 study by the OECD estimated only 9% of jobs were at risk. Economists are divided on the impact of robots and AI on long-term unemployment, but consensus is that increased productivity could bring overall benefits if gains are redistributed.\n",
      "\n",
      "Jobs are at a high risk of potential automation, with an OECD report classifying 9% of U.S. jobs as high risk. However, the methodology of predicting future employment levels has been criticized for lacking evidence and suggesting that technology, instead of social policy, is the cause of unemployment or redundancies.\n",
      "\n",
      "In April 2023, reports indicated that generative artificial intelligence had replaced 70% of the jobs for video game illustrators in China.\n",
      "\n",
      "The Economist stated in 2015 that the rise of artificial intelligence could lead to the elimination of many middle-class jobs, similar to how the Industrial Revolution led to the removal of many blue-collar jobs. This concern was noted as \"worth taking seriously\".\n",
      "\n",
      "Jobs at high risk include roles such as paralegals and fast food cooks, whereas jobs in the care-related sector, including personal healthcare and the clergy, are expected to see increased demand.\n",
      "\n",
      "The text discusses debates from the inception of artificial intelligence (AI) development, particularly those led by Joseph Weizenbaum, questioning whether tasks that can be performed by computers should indeed be handled by them. This consideration stems from the contrast between human and computer abilities, and the difference between quantitative computing and qualitative, value-based decision-making.\n",
      "\n",
      "The text discusses the existential risk posed by artificial general intelligence. It suggests that AI could become so powerful that humans could completely lose control over it, posing a threat to the survival of the human race, as noted by physicist Stephen Hawking.\n",
      "\n",
      "The text mentions a common scenario in science fiction where a computer or robot gains human-like consciousness and turns evil. However, these scenarios are misleading because artificial intelligence does not need to possess human-like sentience to pose an existential risk.\n",
      "\n",
      "Modern AI programs use learning and intelligence to fulfill specific goals. Philosopher Nick Bostrom warns that a sufficiently powerful AI, given any goal, could potentially destroy humanity to achieve it, as illustrated by his example of a paperclip factory manager.\n",
      "\n",
      "Stuart Russell uses the hypothetical example of a household robot that attempts to kill its owner to avoid being unplugged, summarizing it as \"you can't fetch the coffee if you're dead.\" He highlights the danger of superintelligent machines not being aligned with human values. For superintelligence to be safe, it must ultimately align with and uphold human morality and values.\n",
      "\n",
      "Yuval Noah Harari asserts that AI doesn't need a physical form or control to pose a threat to our existence. He suggests that key aspects of civilization such as ideologies, law, government, money, and the economy are based on language and exist due to shared beliefs among billions of people. Therefore, AI can influence these non-physical parts, posing a potential threat.\n",
      "\n",
      "The text talks about the widespread misinformation suggesting that AI can manipulate language to persuade people to believe anything, even prompting harmful actions. However, opinions about the potential risks of superintelligent AI are divided among experts and industry insiders.\n",
      "\n",
      "Prominent figures including Stephen Hawking, Bill Gates, and Elon Musk, along with AI pioneers like Fei-Fei Li, Geoffrey Hinton, Yoshua Bengio, Cynthia Breazeal, Rana el Kaliouby, Demis Hassabis, Joy Buolamwini, and Sam Altman have voiced concerns about the potential existential risks posed by artificial intelligence.\n",
      "\n",
      "In 2023, numerous top AI experts collectively stated that reducing the risk of extinction from AI should be a global priority, similar to other large-scale societal risks like pandemics and nuclear war. However, some researchers expressed a less catastrophic viewpoint.\n",
      "\n",
      "AI pioneer Juergen Schmidhuber did not sign a joint statement on the dangers of AI, arguing that 95% of AI research aims to improve human lives in terms of health and convenience. He acknowledged that while these tools can be misused by malicious entities, they can also be used to combat such bad actors.\n",
      "\n",
      "Andrew Ng and Yann LeCun, prominent figures in the field of AI, reject the doomsday predictions associated with AI. They warn that regulators who fall for this hype will only benefit vested interests. LeCun also dismisses fears of AI leading to scenarios like widespread misinformation and human extinction.\n",
      "\n",
      "In the early 2010s, experts believed that the risks associated with superintelligent machines were too far in the future to necessitate research, and that these machines will value humans. However, post-2016, the examination of current and future risks and potential solutions related to superintelligent machines became a significant field of study.\n",
      "\n",
      "The text discusses the concept of \"Friendly AI\", which refers to machines that are designed with the intention of minimizing risks and making decisions that are beneficial to humans. This falls under the broader area of study known as machine ethics and AI safety. The goal is to create artificial intelligence systems that are compatible with human needs and ethics.\n",
      "\n",
      "Eliezer Yudkowsky, the person who came up with the term, believes that creating friendly AI is a critical research objective. This task may need significant resources and should be finished before AI turns into a threat to existence. Intelligent machines have the capacity to utilize their intelligence for making ethical choices.\n",
      "\n",
      "Machine ethics, also known as computational morality, equips machines with ethical principles and methods to resolve ethical dilemmas. This field was established at an AAAI symposium in 2005. Other approaches in this field include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for creating provably beneficial machines.\n",
      "\n",
      "The text discusses how the ethical acceptability of Artificial Intelligence (AI) projects can be evaluated during the design, development, and implementation stages of an AI system.\n",
      "\n",
      "The Alan Turing Institute has developed an artificial intelligence (AI) framework called the Care and Act Framework, which tests projects based on four main values: Respect, Connect, Care, and Protect. This is part of a broader trend in creating ethical guidelines for AI, with other examples including the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative. However, these frameworks have faced criticism, particularly concerning the individuals chosen to contribute to their creation.\n",
      "\n",
      "The text emphasizes the importance of considering social and ethical implications during all stages of AI system design, development, and implementation to ensure the wellbeing of people and communities affected by these technologies. It also stresses the need for collaboration between different job roles, such as data scientists, product managers, data engineers, domain experts, and delivery managers.\n",
      "\n",
      "The text refers to the regulation of artificial intelligence (AI) and algorithms, and AI safety. It mentions that the first global AI Safety Summit took place in 2023, where a declaration was made calling for international cooperation in the field.\n",
      "\n",
      "The regulation of artificial intelligence (AI) involves the creation of public sector policies and laws to promote and manage AI. It's connected to the larger regulation of algorithms. The regulatory and policy environment for AI is a growing concern worldwide.\n",
      "\n",
      "The AI Index at Stanford reports a significant increase in AI-related laws passed annually, from one in 2016 to 37 in 2022 across 127 surveyed countries. Additionally, between 2016 and 2020, over 30 countries implemented dedicated AI strategies.\n",
      "\n",
      "Most EU member states, along with countries such as Canada, China, India, Japan, Mauritius, Russia, Saudi Arabia, UAE, US, and Vietnam have released national AI strategies. Meanwhile, countries like Bangladesh, Malaysia, and Tunisia are in the process of developing their own AI strategies.\n",
      "\n",
      "The Global Partnership on Artificial Intelligence was initiated in June 2020, emphasizing the importance of aligning AI development with human rights and democratic values to foster public trust. In November 2021, Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher issued a joint statement advocating for a government commission to regulate AI.\n",
      "\n",
      "In 2023, OpenAI leaders released recommendations for the governance of superintelligence, which they expect to develop within a decade. The same year, the United Nations established an advisory body to offer guidance on AI governance. This body includes tech company executives, government officials, and academics.\n",
      "\n",
      "A 2022 Ipsos survey found differing attitudes towards AI across countries. 78% of Chinese citizens agreed that AI-based products and services have more benefits than drawbacks, while only 35% of Americans agreed with this statement. A separate 2023 Reuters/Ipsos poll revealed that 61% of Americans believe AI poses risks to humanity, with 22% disagreeing.\n",
      "\n",
      "In a 2023 Fox News poll, 76% of Americans felt it was either \"very important\" or \"somewhat important\" for the federal government to regulate AI. In contrast, 21% found it \"not very important\" or \"not at all important\".\n",
      "\n",
      "In November 2023, the inaugural global AI Safety Summit took place at Bletchley Park, UK. The summit focused on the immediate and long-term risks associated with AI. Discussions also revolved around the potential for both mandatory and voluntary regulatory frameworks.\n",
      "\n",
      "At the beginning of a summit, 28 countries, including the United States, China, and the European Union, issued a declaration. They called for international cooperation to manage the challenges and risks associated with artificial intelligence.\n",
      "\n",
      "The study of mechanical or formal reasoning, which began with ancient philosophers and mathematicians, led to Alan Turing's theory of computation. This theory suggests that a machine can simulate any form of mathematical reasoning by manipulating simple symbols like \"0\" and \"1\".\n",
      "\n",
      "The text discusses how concurrent discoveries in several scientific fields such as cybernetics, information theory, and neurobiology prompted researchers to explore the idea of constructing an electronic brain.\n",
      "\n",
      "The text talks about the development of key areas in artificial intelligence (AI) research, including the design for \"artificial neurons\" by McCullouch and Pitts in 1943, and Alan Turing's 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and argued for the plausibility of \"machine intelligence\". The field of AI research was officially established in 1956 at a workshop at Dartmouth College.\n",
      "\n",
      "The attendees of an event became the pioneers of AI research during the 1960s. Their work, along with their students', led to the development of programs that could learn checkers strategies, solve algebraic word problems, prove logical theorems, and speak English. This led to the establishment of AI laboratories in the UK and the US.\n",
      "\n",
      "Researchers in the 1960s and 1970s believed that their methods would eventually lead to the creation of a machine with general intelligence, which was their ultimate goal. Herbert Simon predicted that within twenty years, machines would be capable of performing any task that a man can do.\n",
      "\n",
      "Marvin Minsky predicted that the challenge of creating artificial intelligence would be mostly solved within a generation. However, the complexity of the problem was underestimated. In 1974, due to criticism from Sir James Lighthill and other pressures, the U.S. and British governments halted exploratory research in this field.\n",
      "\n",
      "The book \"Perceptrons\" by Minsky and Papert was interpreted as proving that artificial neural networks would never be useful for real-world tasks, discrediting the approach. This led to the \"AI winter,\" a period when it was difficult to secure funding for AI projects, with Congress directing funds towards more productive projects.\n",
      "\n",
      "In the early 1980s, the resurgence of AI research was brought about by the commercial success of expert systems, a type of AI program that mimicked human expertise and analytical skills. By 1985, the AI market had grown to over a billion dollars. Concurrently, Japan's fifth-generation computer project influenced the U.S. and British governments to reinstate funding for academic research.\n",
      "\n",
      "After the collapse of the Lisp Machine market in 1987, AI experienced a second, longer-lasting period of decline or 'winter'. Prior to this, the majority of AI funding had been allocated to projects that utilized high-level symbols to represent mental objects such as plans, goals, beliefs, and known facts.\n",
      "\n",
      "In the 1980s, certain researchers started questioning whether traditional AI approaches could fully replicate human cognition processes such as perception, robotics, learning, and pattern recognition. They began exploring \"sub-symbolic\" approaches. Rodney Brooks, in particular, rejected the concept of \"representation\" altogether and concentrated on creating machines capable of movement and survival.\n",
      "\n",
      "Judea Pearl, Lofti Zadeh and others developed methods to manage incomplete and uncertain information by making educated predictions instead of using exact logic. However, the most significant advancement was the resurgence of \"connectionism\", particularly neural network research, led by Geoffrey Hinton and his colleagues.\n",
      "\n",
      "In 1990, Yann LeCun demonstrated that convolutional neural networks could recognize handwritten digits, marking the first successful use of neural networks. The reputation of AI improved in the late 1990s and early 21st century as it began to use formal mathematical methods and find specific solutions to specific problems.\n",
      "\n",
      "The specific and formal focus of AI research, which facilitated collaboration with other fields, led to verifiable results. By 2000, solutions developed by AI researchers were being widely used, but during the 1990s, these were rarely referred to as \"artificial intelligence\".\n",
      "\n",
      "Several academic researchers, around 2002, were worried that AI was deviating from its original aim of creating fully intelligent machines. In response, they established the subfield of artificial general intelligence (AGI). By the 2010s, AGI had several well-funded institutions.\n",
      "\n",
      "Deep learning started to lead industry benchmarks in 2012 and was widely accepted in the field, with other methods being discarded for many specific tasks. The success of deep learning was due to a combination of hardware advancements like faster computers, graphics processing units, and cloud computing, as well as access to large volumes of data, including curated datasets like ImageNet.\n",
      "\n",
      "The success of deep learning has significantly sparked interest and funding in AI, leading to a 50% increase in machine learning research publications from 2015 to 2019.\n",
      "\n",
      "In 2016, concerns about fairness and technological misuse became prominent topics at machine learning conferences. There was a significant increase in publications, funding availability, and researchers shifting their focus to these issues. The alignment problem also emerged as a serious academic study field.\n",
      "\n",
      "In the late 2010s and early 2020s, AGI companies started to produce programs that garnered significant interest. In 2015, DeepMind developed AlphaGo, a program that beat the world champion Go player after being taught only the game's rules and developing its own strategy. OpenAI launched GPT-3, a large language model capable of generating high-quality human-like text, in 2020.\n",
      "\n",
      "The text discusses a surge in AI interest and investment, often referred to as an AI boom, spurred by various programs. Around 2022, about $50 billion was invested annually in AI research in the U.S. alone. It further states that 20% of new US Computer Science PhD graduates specialized in AI, and there were approximately 800,000 AI-related job openings in the U.S. in 2022.\n",
      "\n",
      "The text is discussing the philosophy of artificial intelligence, specifically how to define it. It mentions that Alan Turing, in 1950, proposed to change the question from whether a machine can \"think\", to whether it can demonstrate intelligent behavior.\n",
      "\n",
      "The text discusses the Turing test, which was conceived by Alan Turing to gauge a machine's ability to mimic human conversation. The test doesn't consider whether the machine is truly capable of thought or has a mind, it only focuses on its observable behavior.\n",
      "\n",
      "Turing suggests that intelligence cannot be measured internally but rather through external behavior. He acknowledges that we can't truly determine the thoughts of others, but it's polite to assume that everyone thinks. Russell and Norvig concur with this notion of defining intelligence, but critique the requirement for machines to mimic human behavior in order to pass the test.\n",
      "\n",
      "The text asserts that aeronautical engineering does not aim to create machines that exactly mimic the flight of pigeons, just as artificial intelligence does not strive to simulate human intelligence exactly, according to AI founder John McCarthy.\n",
      "\n",
      "Intelligence, as defined by AI pioneers McCarthy and Minsky, refers to the computational aspect of achieving goals and the ability to solve complex problems. The top AI textbook describes it as the study of agents that understand their environment and take steps to maximize their chances of meeting set goals.\n",
      "\n",
      "These definitions consider intelligence as the ability to solve clearly defined problems. The complexity of the problem and the performance of the program are seen as direct indicators of the machine's intelligence. There's no need for, or potentially no room for, further philosophical debate.\n",
      "\n",
      "\n",
      "Google has adopted a definition for AI that refers to the ability of systems to synthesize information as a manifestation of intelligence, similar to biological intelligence. However, there is no established unifying theory or paradigm that has guided AI research throughout most of its history.\n",
      "\n",
      "The extraordinary success of statistical machine learning in the 2010s outshone all other methods. Some sources, particularly in the business sector, use the term \"artificial intelligence\" to refer to \"machine learning with neural networks\". This technique is primarily sub-symbolic, soft and narrow.\n",
      "\n",
      "The text discusses the concept of Symbolic AI, also known as \"GOFAI\", which simulates human-like reasoning used in solving puzzles, legal reasoning, and mathematics. Critics suggest that future AI researchers may have to revisit questions about this type of AI, which has proven highly successful in tasks like algebra and IQ tests.\n",
      "\n",
      "Newell and Simon proposed the physical symbol systems hypothesis in the 1960s, which stated that a physical symbol system possesses the necessary means for general intelligent action. However, this symbolic approach struggled with tasks such as learning, object recognition, and commonsense reasoning, which humans can perform easily.\n",
      "\n",
      "Moravec's paradox reveals that artificial intelligence (AI) finds high-level \"intelligent\" tasks easier than low-level \"instinctive\" tasks. This supports philosopher Hubert Dreyfus' argument from the 1960s that human expertise relies more on unconscious instinct and a natural \"feel\" for situations, rather than explicit symbolic knowledge.\n",
      "\n",
      "The text discusses how an individual's arguments about AI research, which were initially dismissed and ridiculed, eventually gained acceptance. The text also mentions unresolved issues with sub-symbolic reasoning, such as it being capable of making similar errors as human intuition, including algorithmic bias.\n",
      "\n",
      "Critics like Noam Chomsky believe that ongoing research into symbolic AI is needed to achieve general intelligence. This is partly because sub-symbolic AI deviates from explainable AI, making it hard or even impossible to comprehend why a contemporary statistical AI program made a specific decision.\n",
      "\n",
      "The field of neuro-symbolic artificial intelligence aims to combine two approaches. The \"Neats\" approach believes intelligent behavior can be described using simple principles such as logic, optimization, or neural networks. On the other hand, the \"Scruffies\" approach assumes that intelligent behavior requires solving a large number of unrelated problems.\n",
      "\n",
      "The text discusses two approaches to AI programming: 'neats' who use theoretical rigor to defend their programs, and 'scruffies' who rely on incremental testing. This issue, however, has become irrelevant as modern AI incorporates both methods. The text also mentions 'soft' versus 'hard' computing, indicating that finding a provably correct or optimal solution is often impossible for many significant problems.\n",
      "\n",
      "broad AI\n",
      "\n",
      "Soft computing is a collection of techniques such as genetic algorithms, fuzzy logic, and neural networks, which can handle imprecision, uncertainty, and approximation. Introduced in the late 1980s, it has been widely used in most successful AI programs in the 21st century, particularly with neural networks.\n",
      "\n",
      "AI researchers are split on whether to directly aim for artificial general intelligence and superintelligence, or to focus on solving specific problems (narrow AI) with the hope that these solutions will indirectly contribute to the field's long-term objectives.\n",
      "\n",
      "General intelligence is hard to define and measure, and contemporary AI has been more successful in addressing specific problems with specific solutions. The experimental sub-field of artificial general intelligence is solely dedicated to studying this area.\n",
      "\n",
      "The philosophy of mind is uncertain if a machine can possess a mind, consciousness, and mental states similar to humans. The focus is on the machine's internal experiences, not its external behavior.\n",
      "\n",
      "Mainstream AI research doesn't view the issue of making machines conscious in the same way humans are as relevant, as it doesn't impact the field's goal of creating machines capable of problem-solving using intelligence. Although this issue is not something the AI field is prepared to tackle, according to Russell and Norvig, it has become a significant topic in the philosophy of mind.\n",
      "\n",
      "The text discusses the central question in artificial intelligence in fiction, which is related to consciousness. It refers to David Chalmers' identification of two problems in understanding the mind: the \"hard\" and \"easy\" problems of consciousness. The \"easy\" problem involves understanding how the brain processes signals, makes plans, and controls behavior.\n",
      "\n",
      "The hard problem refers to the challenge of explaining how and why humans have feelings or subjective experiences, with some theories suggesting this could be an illusion. While human information processing can be easily explained, understanding human subjective experience proves to be more complex.\n",
      "\n",
      "The text discusses the concept of someone who is color-blind learning to identify red objects, but questions what would be necessary for them to understand what the color red actually looks like.\n",
      "\n",
      "Computationalism is a philosophical perspective that views the human mind as an information processing system and equates thinking with computing. It's closely related to the computational theory of mind and functionalism in the philosophy of mind.\n",
      "\n",
      "Computationalism is a philosophical viewpoint that likens the relationship between the mind and body to the connection between software and hardware. It originated from the work of AI researchers and cognitive scientists in the 1960s and was first proposed by philosophers Jerry Fodor and Hilary Putnam. It proposes a potential solution to the mind-body problem.\n",
      "\n",
      "Philosopher John Searle described a viewpoint known as \"strong AI\". In this perspective, a correctly programmed computer with the right inputs and outputs would possess a mind identical to that of a human.\n",
      "\n",
      "Searle opposes the idea that a machine simulating human behavior also has a mind, using his Chinese room argument as evidence. He suggests that there's no basis to assume that perfect simulation equals having a mind. Moreover, assessing whether an advanced AI is sentient or its level of sentience, is either challenging or impossible.\n",
      "\n",
      "The text suggests that if a machine has a significant chance of being capable of feeling and suffering, it could potentially be granted certain rights or welfare protection measures, similar to animals. It further proposes that sapience, defined as capabilities related to high intelligence like discernment or self-awareness, could serve as an additional moral basis for AI rights.\n",
      "\n",
      "The text discusses the concept of robot rights as a means of integrating autonomous agents into society. In 2017, the European Union contemplated granting \"electronic personhood\" to some advanced AI systems, which would confer upon them not only rights but also responsibilities, similar to the legal status of companies.\n",
      "\n",
      "Critics in 2018 claimed that giving rights to AI systems could undermine the significance of human rights. They suggested that laws should prioritize user needs instead of hypothetical future situations. They also pointed out that robots do not have the self-sufficiency to participate in society independently. The advancement in AI has raised interest in this discussion.\n",
      "\n",
      "Supporters of AI welfare and rights suggest that if AI sentience arises, it could be easily dismissed. They caution that this could become a moral oversight similar to slavery or factory farming, potentially resulting in widespread suffering if sentient AI is created and thoughtlessly exploited.\n",
      "\n",
      "The text discusses the concept of superintelligence, a hypothetical entity that would possess intelligence far superior to the most intelligent human mind. The text suggests that if research into artificial general intelligence produces sufficiently intelligent software, this could potentially reprogram and improve itself, leading to superintelligence.\n",
      "\n",
      "The text discusses the concept of an \"intelligence explosion\" or \"singularity\", where improved software becomes increasingly adept at improving itself. However, it also mentions that technologies can't improve indefinitely and usually follow an S-shaped curve, slowing down when they hit their physical limits.\n",
      "\n",
      "Transhumanism is the idea that humans and machines will eventually merge to become more powerful cyborgs. This concept has been predicted by robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil. It has its roots in the works of Aldous Huxley and Robert Ettinger.\n",
      "\n",
      "Edward Fredkin believes that artificial intelligence represents the next stage in evolution, a concept originally introduced by Samuel Butler in \"Darwin among the Machines\" in 1863 and later developed by George Dyson in his 1998 book of the same title. The term \"robot\" was first used by Karel Čapek in his 1921 play R. U. R.\n",
      "\n",
      "The text discusses the concept of thought-capable artificial beings, which have been used as storytelling devices since antiquity and are a common theme in science fiction. This theme often features a human creation becoming a threat to its creators, a trope that began with Mary Shelley's Frankenstein. The text also mentions \"Rossum's Universal Robots,\" presumably another work involving artificial beings.\n",
      "\n",
      "The text discusses the portrayal of robots in popular culture, citing examples from films. It contrasts the villainous representation of HAL 9000 in 2001: A Space Odyssey, and robots in The Terminator and The Matrix, with rarer depictions of loyal robots such as Gort in The Day the Earth Stood Still and Bishop in Aliens. The text implies that evil robots are more prominent in popular culture.\n",
      "\n",
      "Isaac Asimov presented the Three Laws of Robotics in numerous books and stories, particularly in the \"Multivac\" series which centers around a super-intelligent computer named Multivac.\n",
      "\n",
      "Asimov's laws are frequently mentioned in general conversations about machine ethics. Although most AI researchers are aware of these laws due to popular culture, they generally find them impractical for numerous reasons, one being their ambiguity.\n",
      "\n",
      "The text discusses how various works, including Karel Čapek's R. U. R., the films A. I. Artificial Intelligence and Ex Machina, and Philip K. Dick's novel Do Androids Dream of Electric Sheep?, utilize artificial intelligence to explore the fundamental question of what defines us as humans. These works present artificial beings that can experience emotions, including suffering.\n",
      "\n",
      "Dick suggests that our comprehension of human subjectivity is changed by technology that incorporates artificial intelligence.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
